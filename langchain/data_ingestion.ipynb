{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion - Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'lorem.txt'}, page_content='Lorem ipsum is typically a corrupted version of De finibus bonorum et malorum, \\na 1st-century BC text by the Roman statesman and philosopher Cicero, with words altered, added, \\nand removed to make it nonsensical and improper Latin. The first two words themselves are a truncation of dolorem ipsum (\"pain itself\").')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "sample_text = TextLoader('lorem.txt')\n",
    "text_documents = sample_text.load()\n",
    "text_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "sample_pdf = PyPDFLoader('resume.pdf')\n",
    "doc = sample_pdf.load()\n",
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webbased loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.assistents.ai/'}, page_content=\"Home | Products | Use cases | Sales | Customer Prospecting | AI-powered outreach for discovering opportunities and building connections. | Lead Qualification | Instant AI analysis for enriching and engaging high-potential leads. | Sales Performance Optimization | AI-driven insights for strategy, prediction, and sales coaching. | Marketing | Targeted Account Campaigns | AI-precision ABM for hyper-relevant, high-converting campaigns. | Rapid Content Production | AI-assisted content creation across channels in minutes. | Global Market Expansion | Real-time AI translations for effortless market expansion. | Company | Customer Insight Analysis | 360-degree customer views with AI-powered predictive analytics. | Data Quality Management | Continuous AI data cleansing and enrichment across systems. | Business Process Integration | AI-orchestrated tech stack integration for optimized operations. | Get started | Setup 101 | Adding users | Video tutorials | Libraries and SDKs | Adding plugins | Dashboard templates | Resources | Blogs | Explore how you can use AI agents and workflows to scale your business. | Workflow tutorials | Learn how you can use our AI workflows to automate and streamline your business processes | Get started | Setup 101 | Adding users | Video tutorials | Libraries and SDKs | Adding plugins | Dashboard templates | Log in | Log in | Get Started | Blogs & Insights | Stay ahead of the curve and witness exponential business growth with Assistents.ai resources. | Subscribe to our Newsletter | Thanks for signing up! You've been added to the list. | Oops! Something went wrong while submitting the form. | October 24, 2024 | The Evolution of AI Agents: Past, Present, and Future | Explore the evolution of AI agents from their early beginnings to the present and into the future. Learn how these intelligent assistants have transformed from simple rule-based systems to proactive partners in our daily lives and discover what's next for AI agents in this engaging blog. | Read Blog | Latest Blogs | All | AI Workforce | AI Models | Sales | HR | November 9, 2024 | Best Practices in AI Agent Development and Implementation | Learn the top 13 best practices for AI agent development and implementation, including data preparation, scalability, ethical considerations, and more. Boost your AI projects with these expert tips. | Read Blog | November 13, 2024 | 15 AI Agent Development Challenges and How to Overcome Them | Discover the top 15 AI agent development challenges and how to overcome them. Learn effective solutions to tackle common obstacles in AI agent development, from data quality to scalability. | AI Workforce | Read Blog | November 6, 2024 | Agentic AI Components: How Agentic AI Works? | Description: Discover how agentic AI works, including its key components like perception, learning, and autonomous action. Learn the working principles of agentic automation and how it transforms industries. | AI Workforce | Read Blog | October 25, 2024 | What Are AI Agents and How Do They Work? | AI agents are intelligent software programs that perceive their environment, make decisions, and take actions autonomously. | AI Workforce | Read Blog | October 25, 2024 | The Future of Work is Agentic Automation | Discover how agentic automation is transforming the future of work by proactively driving business operations, enhancing efficiency, reducing errors, and empowering employees for strategic innovation. | AI Workforce | Read Blog | October 25, 2024 | AI Agents Fundamentals: Definition, Benefits, and Use Cases | Learn the fundamentals of AI agents, including their definition, key benefits, and use cases. Discover how AI agents transform industries with enhanced efficiency, automation, and intelligent decision-making. | AI Workforce | Read Blog | Load More | Your AI Workforce, | Ready In Minutes | Build, deploy, and integrate AI Agents in minutes with our pre-built templates and workflows. | Schedule a Demo | Transform the way you run your \\xa0Business with AI Agents | Company | Contact Us | Products | AI Agents | Agentic Workflows | Knowledge Base | Integrations | Use Cases | Marketing | Sales | HR | Finance | Operations | Resources | Blogs | Tutorials | © Ampcome LLC - 2024\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader =WebBaseLoader(\n",
    "    web_paths=['https://www.assistents.ai/'],\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"page-wrapper\"),\n",
    "    },\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},\n",
    ")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-10-16', 'Title': 'Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models', 'Authors': 'Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan', 'Summary': 'Large language models (LLMs) have demonstrated impressive reasoning\\nabilities, but they still struggle with faithful reasoning due to knowledge\\ngaps and hallucinations. To address these issues, knowledge graphs (KGs) have\\nbeen utilized to enhance LLM reasoning through their structured knowledge.\\nHowever, existing KG-enhanced methods, either retrieval-based or agent-based,\\nencounter difficulties in accurately retrieving knowledge and efficiently\\ntraversing KGs at scale. In this work, we introduce graph-constrained reasoning\\n(GCR), a novel framework that bridges structured knowledge in KGs with\\nunstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures\\nfaithful KG-grounded reasoning by integrating KG structure into the LLM\\ndecoding process through KG-Trie, a trie-based index that encodes KG reasoning\\npaths. KG-Trie constrains the decoding process, allowing LLMs to directly\\nreason on graphs and generate faithful reasoning paths grounded in KGs.\\nAdditionally, GCR leverages a lightweight KG-specialized LLM for\\ngraph-constrained reasoning alongside a powerful general LLM for inductive\\nreasoning over multiple reasoning paths, resulting in accurate reasoning with\\nzero reasoning hallucination. Extensive experiments on several KGQA benchmarks\\ndemonstrate that GCR achieves state-of-the-art performance and exhibits strong\\nzero-shot generalizability to unseen KGs without additional training.'}, page_content='GRAPH-CONSTRAINED REASONING: FAITHFUL REA-\\nSONING ON KNOWLEDGE GRAPHS WITH LARGE LAN-\\nGUAGE MODELS\\nLinhao Luo1∗, Zicheng Zhao2∗, Chen Gong2, Gholamreza Haffari1, Shirui Pan3†\\n1Monash University 2Nanjing University of Science and Technology 3Griffith University\\n{Linhao.Luo,Gholamreza.Haffari}@monash.edu\\n{zicheng.zhao,chen.gong}@njust.edu.cn, s.pan@griffith.edu.au\\nABSTRACT\\nLarge language models (LLMs) have demonstrated impressive reasoning abilities,\\nbut they still struggle with faithful reasoning due to knowledge gaps and halluci-\\nnations. To address these issues, knowledge graphs (KGs) have been utilized to\\nenhance LLM reasoning through their structured knowledge. However, existing\\nKG-enhanced methods, either retrieval-based or agent-based, encounter difficul-\\nties in accurately retrieving knowledge and efficiently traversing KGs at scale.\\nIn this work, we introduce graph-constrained reasoning (GCR), a novel frame-\\nwork that bridges structured knowledge in KGs with unstructured reasoning in\\nLLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reason-\\ning by integrating KG structure into the LLM decoding process through KG-Trie,\\na trie-based index that encodes KG reasoning paths. KG-Trie constrains the de-\\ncoding process, allowing LLMs to directly reason on graphs and generate faith-\\nful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight\\nKG-specialized LLM for graph-constrained reasoning alongside a powerful gen-\\neral LLM for inductive reasoning over multiple reasoning paths, resulting in ac-\\ncurate reasoning with zero reasoning hallucination. Extensive experiments on\\nseveral KGQA benchmarks demonstrate that GCR achieves state-of-the-art per-\\nformance and exhibits strong zero-shot generalizability to unseen KGs without\\nadditional training. Code is available at https://github.com/RManLuo/\\ngraph-constrained-reasoning.\\n1\\nINTRODUCTION\\nLarge language models (LLMs) have shown impressive reasoning abilities in handling complex\\ntasks (Qiao et al., 2023; Huang & Chang, 2023), marking a significant leap that bridges the gap\\nbetween human and machine intelligence. However, LLMs still struggle with conducting faithful\\nreasoning due to issues of lack of knowledge and hallucination (Huang et al., 2024; Wang et al.,\\n2023). These issues result in factual errors and flawed reasoning processes (Nguyen et al., 2024),\\nwhich greatly undermine the reliability of LLMs in real-world applications.\\nTo address these issues, many studies utilize knowledge graphs (KGs), which encapsulate extensive\\nfactual information in a structured format, to improve the reasoning abilities of LLMs (Pan et al.,\\n2024; Luo et al., 2024). Nevertheless, because of the unstructured nature of LLMs, directly applying\\nthem to reason on KGs is challenging.\\nExisting KG-enhanced LLM reasoning methods can be roughly categorized into two groups:\\nretrieval-based and agent-based paradigms, as shown in Figure 2 (a) and (b). Retrieval-based meth-\\nods (Li et al., 2023; Yang et al., 2024b; Dehghan et al., 2024) retrieve relevant facts from KGs\\nwith an external retriever and then feed them into the inputs of LLMs for reasoning. Agent-based\\nmethods (Sun et al., 2024; Zhu et al., 2024; Jiang et al., 2024) treat LLMs as agents that iteratively\\ninteract with KGs to find reasoning paths and answers.\\n∗Equal Contribution.\\n†Corresponding author.\\n1\\narXiv:2410.13080v1  [cs.CL]  16 Oct 2024\\n67.0%\\n18.0%\\n15.0%\\nFaithful Reasoning Path\\nInvalid - Format Error\\nInvalid - Relation Error\\nFigure\\n1:\\nAnalysis\\nof reasoning errors in\\nRoG (Luo et al., 2024).\\nDespite their success, retrieval-based methods require additional accurate\\nretrievers, which may not generalize well to unseen questions or account\\nfor the graph structure (Mavromatis & Karypis, 2024). Conversely, agent-\\nbased methods necessitate multiple rounds of interaction between agents\\nand KGs, leading to high computational costs and latency (Dehghan et al.,\\n2024). Furthermore, existing works still suffer from serious hallucination\\nissues (Agrawal et al., 2024). Sui et al. (2024) indicates that RoG (Luo\\net al., 2024), a leading KG-enhanced reasoning method, still experiences\\n33% hallucination errors during reasoning on KGs, as shown in Figure 1.\\nTo this end, we introduce graph-constrained reasoning (GCR), a novel KG-\\nguided reasoning paradigm that connects unstructured reasoning in LLMs\\nwith structured knowledge in KGs, seeking to eliminate hallucinations dur-\\ning reasoning on KGs and ensure faithful reasoning. Inspired by the con-\\ncept that LLMs reason through decoding (Wei et al., 2022), we incorporate\\nthe KG structure into the LLM decoding process. This enables LLMs to directly reason on graphs\\nby generating reliable reasoning paths grounded in KGs that lead to correct answers.\\nIn GCR, we first convert KG into a structured index, KG-Trie, to facilitate efficient reasoning on KG\\nusing LLM. Trie is also known as the prefix tree (Wikipedia contributors, 2024) that compresses a\\nset of strings, which can be used to restrict LLM output tokens to those starting with valid prefixes\\n(De Cao et al., 2022; Xie et al., 2022). KG-Trie encodes the reasoning paths in KGs as formatted\\nstrings to constrain the decoding process of LLMs. Then, we propose graph-constrained decoding\\nthat employs a lightweight KG-specialized LLM to generate multiple KG-grounded reasoning paths\\nand hypothesis answers. With the constraints from KG-Trie, we ensure faithful reasoning while\\nleveraging the strong reasoning capabilities of LLMs to efficiently explore paths on KGs in constant\\ntime. Finally, we input multiple generated reasoning paths and hypothesis answers into a powerful\\ngeneral LLM to utilize its inductive reasoning ability to produce final answers. In this way, GCR\\ncombines the graph reasoning strength of KG-specialized LLMs and the inductive reasoning advan-\\ntage in general LLMs to achieve faithful and accurate reasoning on KGs. The main contributions of\\nthis work are as follows:\\n• We propose a novel framework called graph-constrained reasoning (GCR) that bridges the\\ngap between structured knowledge in KGs and unstructured reasoning in LLMs, allowing\\nfor efficient reasoning on KGs via LLM decoding.\\n• We combine the complementary strengths of a lightweight KG-specialized LLM with a\\npowerful general LLM to enhance reasoning performance by leveraging their respective\\ngraph-based reasoning and inductive reasoning capabilities.\\n• We conduct extensive experiments on several KGQA reasoning benchmarks, demonstrat-\\ning that GCR not only achieves state-of-the-art performance with zero hallucination, but\\nalso shows zero-shot generalizability for reasoning on unseen KGs without additional train-\\ning.\\n2\\nRELATED WORK\\nLLM reasoning. Many studies have been proposed to analyze and improve the reasoning ability\\nof LLMs (Wei et al., 2022; Wang et al., 2024; Yao et al., 2024). To elicit the reasoning ability\\nof LLMs, Chain-of-thought (CoT) reasoning (Wei et al., 2022) prompts the model to generate a\\nchain of reasoning steps in response to a question. Wang et al. (2024) propose a self-consistency\\nmechanism that generates multiple reasoning paths and selects the most consistent answer across\\nthem. The tree-of-thought (Yao et al., 2024) structures reasoning as a branching process, exploring\\nmultiple steps in a tree-like structure to find optimal solutions. Other studies focus on fine-tuning\\nLLMs on various reasoning tasks to improve reasoning abilities (Yu et al., 2022; Hoffman et al.,\\n2024). For instance, OpenAI (2024c) adopts reinforcement learning to train their most advanced\\nLLMs called “OpenAI o1” to perform complex reasoning, which produces a long internal chain of\\nthought before final answers.\\nKG-enhanced LLM reasoning. To mitigate the knowledge gap and hallucination issues in LLM\\nreasoning, research incorporates KGs to enhance LLM reasoning (Pan et al., 2024). KD-CoT (Wang\\n2\\n# Reasoning Path:\\n# Answer:\\nMelania Trump\\nGeneral\\nLLM\\nKG-specialized\\nLLM\\nQ\\nA\\nQuestion\\nAnswer\\nKnowledge\\nGraph\\n(a) Retrieval-based LLM Reasoning\\nLLM\\nReasoning\\n(b) Agent-based LLM Reasoning\\n(c) Ours: Knowledge Graph-constrained LLM Reasoning\\nQ:\\xa0Who is\\nthe spouse\\nof the ex-\\npresident of\\nUSA?\\nt=2\\nA: Based on the paths,\\nthe answers are: Laura\\nBush, Michelle Obama,\\nMelania Trump.\\nKG-specialized\\nLLM\\nKG-Trie\\nConstraint\\xa0\\n①\\xa0Offline KG-Trie\\nConstruction\\n②\\xa0Graph-constrained\\nDecoding\\nt=1\\nReasoning Paths and\\xa0\\n\\xa0\\xa0Hypothesis\\xa0Answers\\nGeneral\\nLLM\\n③\\xa0Inductive\\nReasoning\\n# Reasoning Path:\\n# Answer:\\nLaura Bush\\n# Reasoning Path:\\n# Answer:\\nMichelle Obama\\nQ\\nKnowledge\\nRetriever\\nA\\nRetrieved\\nFacts\\nt=1\\nLLM\\nEx-president\\nFounded_in\\n1776\\nUSA\\nBarack Obama\\nBorn_in\\nHonolulu\\nMichelle\\nObama\\nSasha Obama\\nMorther_of\\nLLM Agent\\nA\\nSpouse_of\\nt=1\\nt=2\\nt=3\\nT Steps\\nUSA\\nDonald Trump\\nEx-president\\nMichelle\\nObama\\nGeorge W.\\nBush\\xa0\\nEx-president\\nBarack Obama\\nEx-president\\nSpouse_of\\n1776\\nFounded_in\\nWashington\\nD.C.\\nCapital\\nLaura\\nBush\\xa0\\nKnowledge Graph\\nMelania\\nTrump\\nMarry_to\\nIvana\\nTrump\\nEx-wife\\nSpouse_of\\nQ\\nFigure 2: Illustration of existing KG-enhanced LLM reasoning paradigms and proposed graph-\\nconstrained reasoning (GCR). 1) First, given a KG, we convert it into the KG-Trie, serving as a\\nstructured index to facilitate efficient reasoning path searches using LLMs. 2) Then, we design a\\ngraph-constrained decoding process that employs a lightweight KG-specialized LLM to generate\\nmultiple KG-grounded reasoning paths and hypothesis answers. This ensures the faithfulness of the\\nreasoning process while leveraging the strong capabilities of LLMs to efficiently explore reasoning\\npaths within KGs. 3) Finally, we input the generated reasoning paths and hypothesis answers into a\\npowerful general LLM to utilize its inductive reasoning ability to produce final answers.\\net al., 2023) retrieve facts from an external knowledge graph to guide the CoT performed by LLMs.\\nRoG (Luo et al., 2024) proposes a planning-retrieval-reasoning framework that retrieves reasoning\\npaths from KGs to guide LLMs conducting faithful reasoning. To capture graph structure, GNN-\\nRAG (Mavromatis & Karypis, 2024) adopts a lightweight graph neural network to effectively re-\\ntrieve from KGs. Instead of retrieving, StructGPT (Jiang et al., 2023) and ToG (Sun et al., 2024)\\ntreat LLMs as agents to interact with KGs to find reasoning paths leading to the correct answers.\\n3\\nPRELIMINARY\\nKnowledge Graphs (KGs) represent a wealth of factual knowledge as a collection of triples: G =\\n{(e, r, e′) ∈E × R × E}, where E and R denote the set of entities and relations, respectively.\\nReasoning Paths are sequences of consecutive triples in KGs: wz = e0\\nr1\\n−→e1\\nr2\\n−→. . .\\nrl\\n−→el,\\nwhere ∀(ei−1, ri, ei) ∈G. The paths reveal the connections between knowledge that potentially\\nfacilitate reasoning. For example, the reasoning path: wz = Alice\\nmarry to\\n−−−−−−→Bob\\nfather of\\n−−−−−−→\\nCharlie indicates that “Alice” is married to “Bob” and “Bob” is the father of “Charlie”. Therefore,\\n“Alice” could be reasoned to be the mother of “Charlie”.\\nKnowledge Graph Question Answering (KGQA) is a representative reasoning task with the as-\\nsistance of KGs. Given a natural language question q and a KG G, the task aims to design a function\\nf to reason answers a ∈A based on knowledge from G, i.e., a = f(q, G).\\n3\\n4\\nAPPROACH\\n4.1\\nFROM CHAIN-OF-THOUGHT REASONING TO GRAPH-CONSTRAINED REASONING\\nChain-of-Thought Reasoning (CoT) (Wei et al., 2022) has been widely adopted to enhance the\\nreasoning ability of LLMs by autoregressively generating a series of reasoning steps leading to the\\nanswer. Specifically, given a question q, CoT models the joint probability of the answer a and\\nreasoning steps z as\\nP(a|q) =\\nX\\nz\\nPθ(a|z, q)Pθ(z|q) =\\nX\\nz\\nPθ(a|q, z)\\n|z|\\nY\\ni=1\\nPθ(zi|q, z1:i−1),\\n(1)\\nwhere q denotes the input question, a denotes the final answer, θ denotes the parameters of LLMs,\\nand zi denotes the i-th step of the reasoning process z. To further enhance the reasoning ability,\\nmany previous works focus on improving the reasoning process Pθ(z|q) by exploring and aggregat-\\ning multiple reasoning processes (Wang et al., 2024; Yao et al., 2024).\\nDespite the effectiveness, a major issue remains the faithfulness of the reasoning process generated\\nby LLMs (Huang et al., 2024). The reasoning is represented as a sequence of tokens decoded\\nstep-by-step, which can accumulate errors and result in hallucinated reasoning paths and answers\\n(Nguyen et al., 2024). To address these issues, we utilize knowledge graphs (KGs) to guide LLMs\\ntoward faithful reasoning.\\nKG-enhanced Reasoning utilizes the structured knowledge in KGs to improve the reasoning of\\nLLMs (Luo et al., 2024; Sun et al., 2024), which can generally be expressed as finding a reasoning\\npath wz on KGs that connects the entities mentioned in the question and the answer. This can be\\nformulated as\\nP(a|q, G) =\\nX\\nwz\\nPϕ(a|q, wz)Pϕ(wz|q, G),\\n(2)\\nwhere Pϕ(wz|q, G) denotes the probability of discovering a reasoning path wz on KGs G given the\\nquestion q by a function parameterized by ϕ. To acquire reasoning paths for reasoning, most prior\\nstudies follow the retrieval-based (Li et al., 2023) or agent-based paradigm (Sun et al., 2024), as\\nshown in Figure 2 (a) and (b), respectively. Nevertheless, retrieval-based methods rely on precise\\nadditional retrievers, while agent-based methods are computationally intensive and lead to high\\nlatency. To address these issues, we propose a novel graph-constrained reasoning paradigm (GCR).\\nGraph-constrained Reasoning (GCR) directly incorporates KGs into the decoding process of\\nLLMs to achieve faithful reasoning. The overall framework of GCR is illustrated in Figure 2 (c),\\nwhich consists of three main components: 1) Knowledge Graph Trie Construction: building a\\nstructural index of KG to guide LLM reasoning, 2) Graph-constrained Decoding: generating KG-\\ngrounded paths and hypothesis answers using LLMs, and 3) Graph Inductive Reasoning: reasoning\\nover multiple paths and hypotheses to derive final answers.\\n4.2\\nKNOWLEDGE GRAPH TRIE CONSTRUCTION\\nKnowledge graphs (KGs) store abundant knowledge in a structured format. However, large language\\nmodels (LLMs) struggle to efficiently access and reason on KGs due to their unstructured nature. To\\naddress this issue, we propose to convert KGs into knowledge graph Tries (KG-Tries), which serve\\nas a structured index of KGs to facilitate efficient reasoning on graphs using LLMs.\\nA Trie (a.k.a. prefix tree) (Wikipedia contributors, 2024; Fredkin, 1960) is a tree-like data structure\\nthat stores a dynamic set of strings, where each node represents a common prefix of its children.\\nTries can be used to restrict LLM output tokens to those starting with valid prefixes (De Cao et al.,\\n2022; Xie et al., 2022; Chen et al., 2022). The tree structure of Trie is an ideal choice for encoding\\nthe reasoning paths in KGs for LLMs to efficiently traverse.\\nWe first adopt the breadth-first search (BFS) algorithm to retrieve reasoning paths Wz within L hops\\nstarting from entities mentioned in the questions. The retrieved paths are formatted as sentences\\nusing the template shown in Figure 7. The formatted sentences are then split into tokens by the\\n4\\ntokenizer of LLM and stored as a KG-Trie CG. The overall process can be formulated as:\\nWz = BFS(G, {eq}, L),\\n(3)\\nTz = Tokenizer(Wz),\\n(4)\\nCG = Trie(Tz),\\n(5)\\nwhere eq denotes the entities mentioned in the question, L denotes the maximum hops of paths, and\\nTz denotes the tokens of reasoning paths. The KG-Trie CG is used as a constraint to guide the LLM\\ndecoding process.\\nBy constructing KG-Trie for each question entity, we can enable efficient traversal of reasoning paths\\nin constant time (O(|Wz|)) without costly graph traversal (Sun et al., 2024). Moreover, KG-Trie can\\nbe pre-constructed offline and loaded during reasoning. This significantly reduces the computational\\ncost and latency of reasoning on KGs, making it feasible for real-time applications.\\n============================= Prompt Input ================================\\nPlease generate some reasoning paths in the KG starting from the topic entities to answer the question.\\n# Question: what is the name of justin bieber brother?\\n============================= LLM Output ================================\\n# Reasoning Path: <PATH> Justin Bieber →people.person.parents →Jeremy Bieber →peo-\\nple.person.children →Jaxon Bieber </PATH>\\n# Answer: Jaxon Bieber\\nFigure 3: An example of the graph-constrained decoding. Detailed prompts can be found in Figure 8.\\n4.3\\nGRAPH-CONSTRAINED DECODING\\nLarge language models (LLMs) have strong reasoning capabilities but still suffer from severe hal-\\nlucination issues, which undermines the trustworthiness of the reasoning process. To tackle this\\nissue, we propose graph-constrained decoding, which unifies the reasoning ability of LLMs with the\\nstructured knowledge in KGs to generate faithful KG-grounded reasoning paths leading to answers.\\nGiven a question q, we design an instruction prompt to harness the reasoning ability of LLMs to\\ngenerate reasoning paths wz and hypothesis answers a. To eliminate the hallucination during rea-\\nsoning on KGs, we adopt the KG-Trie CG as constraints to guide the decoding process of LLMs and\\nonly generate reasoning paths that are valid in KGs, formulated as:\\nPϕ(a, wz|q) = Pϕ(a|q, wz)\\n|\\n{z\\n}\\nRegular decoding\\nGraph-constrained decoding\\nz\\n}|\\n{\\n|wz|\\nY\\ni=1\\nPϕ(wzi|q, wz1:i−1)CG(wzi|wz1:i−1),\\n(6)\\nCG(wzi|wz1:i−1) =\\n\\x1a1, ∃prefix(wz1:i, wz), ∃wz ∈Wz,\\n0, else,\\n(7)\\nwhere wzi denotes the i-th token of the reasoning path wz, Pϕ denotes the token probabilities\\npredicted by the LLM with parameters ϕ, and CG(wzi|wz1:i−1) denotes the constraint function that\\nchecks whether the generated tokens wz1:i is a valid prefix of the reasoning path using KG-Trie.\\nAfter a valid reasoning path is generated, we switch back to the regular decoding process to generate\\na hypothesis answer conditioned on the path.\\nTo further enhance KG reasoning ability, we fine-tune a lightweight KG-specialized LLM with\\nparameters ϕ on the graph-constrained decoding task. Specifically, given a question q, the LLM is\\noptimized to generate relevant reasoning paths wz that are helpful for answering the question, then\\nprovide a hypothesis answer a based on it, which can be formulated as:\\nL = E(q,wz,a)∼DG log Pϕ(a, wz|q) = E\\n\\uf8ee\\n\\uf8f0log\\n|a|\\nY\\ni=1\\nPϕ(ai|q, wz, a1:i−1)\\n|wz|\\nY\\nj=1\\nPϕ(wzj|q, wz1:j−1)\\n\\uf8f9\\n\\uf8fb,\\n(8)\\nwhere ai and wzj denote the i-th token of the answer a and the j-th token of the reasoning path wz,\\nrespectively.\\n5\\nThe training data (q, wz, a) ∈DG consists of question-answer pairs and reasoning paths generated\\nfrom KGs. We use the shortest paths connecting the entities in the question and answer as the\\nreasoning path wz for training, where details can be found in Section 7. An example of graph-\\nconstrained decoding is illustrated in Figure 3, where <PATH> and </PATH> are special tokens\\nto control the start and end of graph-constrained decoding. Experiment results in Section 5.2 show\\nthat even a lightweight KG-specialized LLM (0.5B) can achieve satisfactory performance in KG\\nreasoning.\\nThe graph-constrained decoding method differs from retrieval-based methods by integrating a pre-\\nconstructed KG-Trie into the decoding process of LLMs. This not only reduces input tokens, but\\nalso bridges the gap between unstructured reasoning in LLMs and structured knowledge in KGs,\\nallowing for efficient reasoning on KGs regardless of its scale, which results in faithful reasoning\\nleading to answers. Additionally, experimental results in Section 5.4 demonstrate that KG-Trie can\\nintegrate with new KGs on the fly, showcasing its zero-shot generalizability for reasoning on unseen\\nKGs without further training.\\n4.4\\nGRAPH INDUCTIVE REASONING\\nGraph-constrained decoding harnesses the reasoning ability of a KG-specialized LLM to generate a\\nfaithful reasoning path and a hypothesis answer. However, complex reasoning tasks typically admit\\nmultiple reasoning paths that lead to correct answers (Stanovich et al., 2000). Incorporating diverse\\nreasoning paths would be beneficial for deliberate thinking and reasoning (Evans, 2010; Wang et al.,\\n2024). To this end, we propose to input multiple reasoning paths and hypothesis answers generated\\nby the KG-specialized LLM into a powerful general LLM to leverage its inductive reasoning ability\\nto produce final answers.\\nThe graph-constrained decoding seamlessly integrates into the decoding process of LLMs, allowing\\nit to be paired with various LLM generation strategies like beam-search (Federico et al., 1995) to\\ntake advantage of the GPU parallel computation. Thus, given a question, we adopt graph-constrained\\ndecoding to simultaneously generate K reasoning paths and hypothesis answers with beam search in\\na single LLM call, which are then inputted into a general LLM to derive final answers. The overall\\nprocess can be formulated as:\\nZK = {ak, wk\\nz}K\\nk=1 = arg top-K Pϕ(a, wz|q),\\n(9)\\nPθ(A|q, ZK) ≃\\nK\\nY\\nk=1\\nPθ(A|q, ak, wk\\nz),\\n(10)\\nwhere θ denotes the parameters of the general LLM, ZK denotes the set of top-K reasoning paths\\nand hypothesis answers, and A denotes the final answers.\\nWe follow the FiD framework (Izacard & Grave, 2021; Singh et al., 2021) to incorporate multiple\\nreasoning paths and hypothesis answers to conduct inductive reasoning within one LLM call, i.e.,\\nPθ(A|q, ZK), where detailed prompts can be found in Figure 9. The general LLM can be any\\npowerful LLM, such as ChatGPT (OpenAI, 2022), or Llama-3 (Meta, 2024), which can effectively\\nleverage their internal reasoning ability to reason over multiple reasoning paths to produce final\\nanswers without additional fine-tuning.\\n5\\nEXPERIMENT\\nIn our experiments, we aim to answer the following research questions: RQ1: Can GCR achieve\\nstate-of-the-art reasoning performance with balances between efficiency and effectiveness? RQ2:\\nCan GCR eliminate hallucinations and conduct faithful reasoning? RQ3: Can GCR generalize to\\nunseen KGs on the fly?\\n5.1\\nEXPERIMENT SETUPS\\nDatasets. Following previous research (Luo et al., 2024; Sun et al., 2024), we first evaluate the\\nreasoning ability of GCR on two benchmark KGQA datasets: WebQuestionSP (WebQSP) (Yih et al.,\\n2016) and Complex WebQuestions (CWQ) (Talmor & Berant, 2018). Freebase (Bollacker et al.,\\n6\\nTable 1: Performance comparison with different baselines on the two KGQA datasets.\\nTypes\\nMethods\\nWebQSP\\nCWQ\\nHit\\nF1\\nHit\\nF1\\nLLM Reasoning\\nQwen2-0.5B (Yang et al., 2024a)\\n26.2\\n17.2\\n12.5\\n11.0\\nQwen2-1.5B (Yang et al., 2024a)\\n41.3\\n28.0\\n18.5\\n15.7\\nQwen2-7B (Yang et al., 2024a)\\n50.8\\n35.5\\n25.3\\n21.6\\nLlama-2-7B (Touvron et al., 2023)\\n56.4\\n36.5\\n28.4\\n21.4\\nLlama-3.1-8B (Meta, 2024)\\n55.5\\n34.8\\n28.1\\n22.4\\nGPT-4o-mini (OpenAI, 2024a)\\n63.8\\n40.5\\n63.8\\n40.5\\nChatGPT (OpenAI, 2022)\\n59.3\\n43.5\\n34.7\\n30.2\\nChatGPT+Few-shot (Brown et al., 2020)\\n68.5\\n38.1\\n38.5\\n28.0\\nChatGPT+CoT (Wei et al., 2022)\\n73.5\\n38.5\\n47.5\\n31.0\\nChatGPT+Self-Consistency (Wang et al., 2024)\\n83.5\\n63.4\\n56.0\\n48.1\\nGraph Reasoning\\nGraftNet (Sun et al., 2018)\\n66.7\\n62.4\\n36.8\\n32.7\\nNSM (He et al., 2021)\\n68.7\\n62.8\\n47.6\\n42.4\\nSR+NSM (Zhang et al., 2022)\\n68.9\\n64.1\\n50.2\\n47.1\\nReaRev (Mavromatis & Karypis, 2022)\\n76.4\\n70.9\\n52.9\\n47.8\\nKG+LLM\\nKD-CoT (Wang et al., 2023)\\n68.6\\n52.5\\n55.7\\n-\\nEWEK-QA (Dehghan et al., 2024)\\n71.3\\n-\\n52.5\\n-\\nToG (ChatGPT) (Sun et al., 2024)\\n76.2\\n-\\n57.6\\n-\\nToG (GPT-4) (Sun et al., 2024)\\n82.6\\n-\\n68.5\\n-\\nEffiQA (Dong et al., 2024)\\n82.9\\n-\\n69.5\\nRoG (Llama-2-7B) (Luo et al., 2024)\\n85.7\\n70.8\\n62.6\\n56.2\\nGNN-RAG (Mavromatis & Karypis, 2024)\\n85.7\\n71.3\\n66.8\\n59.4\\nGNN-RAG+RA (Mavromatis & Karypis, 2024)\\n90.7\\n73.5\\n68.7\\n60.4\\nGCR (Llama-3.1-8B + ChatGPT)\\n92.6\\n73.2\\n72.7\\n60.9\\nGCR (Llama-3.1-8B + GPT-4o-mini)\\n92.2\\n74.1\\n75.8\\n61.7\\n2008) is adopted as the knowledge graph for both datasets. To further evaluate the generalizability\\nof GCR, we conduct zero-shot transfer experiments on two new KGQA datasets: CommonsenseQA\\n(CSQA) (Talmor et al., 2019) and MedQA-USMLE (MedQA) (Jin et al., 2021). For CSQA, we use\\nConceptNet (Speer et al., 2017) as the KG, while for MedQA, we use a medical KG constructed\\nfrom the Unified Medical Language System (Yasunaga et al., 2021). The details of the datasets are\\ndescribed in Section 7.\\nBaselines. We compare GCR with the 22 baselines grouped into three categories: 1) LLM reasoning\\nmethods, 2) graph reasoning methods, and 3) KG-enhanced LLM reasoning methods. The detailed\\nbaselines are listed in Section 8.\\nEvaluation Metrics. We adopt Hit and F1 as the evaluation metrics following previous works (Luo\\net al., 2024; Sun et al., 2024) on WebQSP and CWQ. Hit checks whether any correct answer exists in\\nthe generated predictions, while F1 considers the coverage of all answers by balancing the precision\\nand recall of predictions. Because CSQA and MedQA are multiple-choice QA datasets, we adopt\\naccuracy as the evaluation metric.\\nImplementations. For GCR, we use the KG-Trie to index all the reasoning paths within 2 hops\\nstarting from question entities. For the LLMs, we use a fine-tuned Llama-3-8B (Meta, 2024) as\\nthe KG-specialized LLM. We generate top-10 reasoning paths and hypothesis answers from graph-\\nconstrained decoding. We adopt the advanced ChatGPT (OpenAI, 2022) and GPT-4o-mini (OpenAI,\\n2024a) as the general LLMs for inductive reasoning. The detailed hyperparameters and experiment\\nsettings are described in Section 9.\\n5.2\\nRQ1: REASONING PERFORMANCE AND EFFICIENCY\\nMain Results. In this section, we compare GCR with other baselines on KGQA benchmarks to\\nevaluate the reasoning performance. From the results shown in Table 1, GCR achieves the best\\nperformance on both datasets, outperforming the second-best by 2.1% and 9.1% in terms of Hit on\\nWebQSP and CWQ, respectively. The results demonstrate that GCR can effectively leverage KGs to\\nenhance LLMs and achieve state-of-the-art reasoning performance.\\nAmong the LLM reasoning methods, ChatGPT with self-consistency prompts demonstrates the best\\nperformance, which indicates the powerful reasoning ability inherent in LLMs. However, their per-\\nformances are still limited by the model size and complex reasoning required over structured data.\\nGraph reasoning methods, such as ReaRev, achieve competitive performance on WebQSP by ex-\\n7\\nTable 2: Efficiency and performance comparison of different methods on WebQSP.\\nTypes\\nMethods\\nHit\\nAvg. Runtime (s)\\nAvg. # LLM Calls\\nAvg. # LLM Tokens\\nRetrieval-based\\nS-Bert\\n66.9\\n0.87\\n1\\n293\\nBGE\\n72.7\\n1.05\\n1\\n357\\nOpenAI-Emb.\\n79.0\\n1.77\\n1\\n330\\nGNN-RAG\\n85.7\\n1.52\\n1\\n414\\nRoG\\n85.7\\n2.60\\n2\\n521\\nAgent-based\\nToG\\n75.1\\n16.14\\n11.6\\n7,069\\nEffiQA\\n82.9\\n-\\n7.3\\n-\\nOurs\\nGCR\\n92.6\\n3.60\\n2\\n231\\nplicitly modeling the graph structure. But they struggle to generalize across different datasets and\\nunderperform on CWQ. In KG+LLM methods, both agent-based methods (e.g., ToG, EffiQA) and\\nretrieval-based methods (e.g., RoG, GNN-RAG) achieve the second-best performance. Neverthe-\\nless, they still suffer from inefficiency and reasoning hallucinations which limit their performance.\\nIn contrast, GCR effectively eliminates hallucinations and conducts faithful reasoning by leveraging\\nthe structured KG index and graph-constrained decoding.\\nEfficiency Analysis.\\nTo show the efficiency of GCR, we compare the average runtime, number of LLM calls, and num-\\nber of input tokens with retrieval-based and agent-based methods in Table 2. For retrieval-based\\nmethods, we compare with dense retrievers (e.g., S-Bert (Reimers & Gurevych, 2019), BGE (Zhang\\net al., 2023), OpenAI-Emb. (OpenAI, 2024b)) and graph-based retrievers (e.g., GNN-RAG (Mavro-\\nmatis & Karypis, 2024), RoG (Luo et al., 2024)), which retrieve reasoning paths from KGs and feed\\nthem into LLMs for reasoning answers. For agent-based methods, we compare with ToG (Sun et al.,\\n2024) and EffiQA1 (Dong et al., 2024), which heuristically search on KGs for answers. The detailed\\nsettings are described in Section 9.\\nDense retrievers are most efficient in terms of runtime and LLM calls as they convert all paths into\\nsentences and encode them as embeddings in advance. However, they sacrifice their accuracy in\\nretrieving as they are not designed to encode graph structure. Graph-based retrievers and agent-\\nbased methods achieve better performance by considering graph structure; however, they require\\nmore time and LLM calls. Specifically, the retrieved graph is fed as inputs to LLMs, which leads to\\na large number of input tokens. Agent-based methods, like ToG, require more LLM calls and input\\ntokens as the question difficulty increases due to their iterative reasoning process. In contrast, GCR\\nachieves the best performance with a reasonable runtime and number of LLM calls. With the help\\nof KG-Trie, GCR explores multiple reasoning paths at the same time during the graph-constrained\\ndecoding, which does not involve additional LLM calls or input tokens and benefits from the parallel\\nGPU computation with low latency. More efficiency analysis under different beam sizes used for\\ngraph-constrained decoding can be found in parameter analysis.\\nTable 3: Ablation studies of GCR on two KGQA datasets.\\nVariants\\nWebQSP\\nCWQ\\nF1\\nPrecision\\nRecall\\nF1\\nPrecision\\nRecall\\nGCR (Llama-3.1-8B + ChatGPT)\\n73.2\\n80.0\\n76.9\\n60.9\\n61.1\\n66.6\\nGCR w/o KG-specialized LLM\\n52.9\\n66.3\\n50.2\\n37.5\\n40.8\\n37.9\\nGCR w/o General LLM\\n57.0\\n58.0\\n70.1\\n39.4\\n32.8\\n64.3\\nAblation Study. We first conduct an\\nablation study to analyze the effec-\\ntiveness of the KG-specialized LLM\\nand general LLM in GCR. As shown\\nin Table 3, the full GCR achieves the\\nbest performance on both datasets.\\nBy removing the KG-specialized LLM, we feed all 2-hop reasoning paths into the general LLM.\\nThis results in a significant performance drop, indicating its importance in utilizing reasoning abil-\\nity to find relevant paths on KGs for reasoning. On the other hand, removing the general LLM\\nand relying solely on answers predicted by KG-specialized LLM leads to a noticeable decrease in\\nprecision, due to noises in its predictions. This highlighting the necessity of the general LLM for\\nconducting inductive reasoning over multiple paths to derive final answers.\\nDifferent LLMs. We further analyze LLMs used for KG-specialized and general LLMs in Table 4.\\nFor KG-specialized LLMs, we directly plug the KG-Trie into different LLMs to conduct graph-\\nconstrained decoding and use the same general LLM for final reasoning. For general LLMs, we\\nadopt the same reasoning paths generated by KG-specialized LLMs to different LLMs to produce\\nfinal answers. For zero-shot and few-shot learning, we adopt the original LLMs without fine-tuning,\\nwhose prompt templates can be found in Figures 8 and 10.\\n1Since there is no available code for EffiQA, we directly copy the results from the original paper.\\n8\\nTable 4: Comparison of different LLMs used in\\nGCR on WebQSP.\\nComponents\\nLearning Types\\nVariants\\nHit\\nF1\\nKG-specialized\\nLLM\\nZero-shot\\nLlama-3.1-8B\\n28.25\\n10.32\\nLlama-3.1-70B\\n38.53\\n12.53\\nFew-shot\\nLlama-3.1-8B\\n33.24\\n11.19\\nLlama-3.1-70B\\n41.13\\n13.14\\nFine-tuned\\nQwen2-0.5B\\n87.48\\n60.03\\nQwen2-1.5B\\n89.21\\n62.97\\nQwen2-7B\\n92.31\\n72.74\\nLlama-2-7B\\n92.55\\n73.23\\nLlama-3.1-8B\\n92.74\\n73.14\\nGeneral LLM\\nZero-shot\\nQwen-2-7B\\n86.32\\n67.59\\nLlama-3.1-8B\\n90.24\\n71.19\\nLlama-3.1-70B\\n90.24\\n71.19\\nChatGPT\\n92.55\\n73.23\\nGPT-4o-mini\\n92.23\\n74.05\\nResults in Table 4 show that a lightweight LLM\\n(0.5B) can outperform a large one (70B) af-\\nter fine-tuning, indicating the effectiveness of\\nfine-tuning in enhancing the ability of LLMs\\nand make them specialized for KG reason-\\ning.\\nHowever, the larger LLMs (e.g., 7B\\nand 8B) still perform better than smaller ones,\\nhighlighting the importance of model capac-\\nity in searching relevant reasoning paths on\\nKGs.\\nSimilar trends are observed in gen-\\neral LLMs where larger models (e.g., GPT-4o-\\nmini and ChatGPT) outperform smaller ones\\n(e.g., Qwen-2-7B and Llama-3.1-8B), show-\\ncasing their stronger inductive reasoning abili-\\nties. This further emphasizes the need of paring\\npowerful general LLMs with lightweight KG-specialized LLMs to achieve better reasoning driven\\nby both of them.\\n1\\n3\\n5\\n10\\n20\\nGraph-constrained decoding beam size K\\n0\\n2\\n4\\n6\\n8\\nGeneration Time (s)\\n40\\n50\\n60\\n70\\n80\\n90\\nAnswer Coverage (%)\\nGeneration Time (s)\\nHit\\nF1\\nPrecision\\nRecall\\nFigure 4: Parameter analysis of beam\\nsize K.\\nParameter Analysis. We first analyze the impact of dif-\\nferent beam sizes K for graph-constrained decoding on\\nthe performance of GCR. We conduct the experiments on\\nWebQSP with different beam sizes of 1, 3, 5, 10, and 20.\\nThe results are shown in Figure 4. We observe that the hit\\nand recall of GCR increase with the beam size. Because,\\nwith a larger beam size, the LLMs can explore more rea-\\nsoning paths and find the correct answers. However, the\\nF1 score, peaks when the beam size is set to 10. This is\\nbecause the beam size of 10 can provide a balance be-\\ntween the exploration and exploitation of the reasoning\\npaths. When the beam size is set to 20, the performance\\ndrops due to the increased complexity of the search space,\\nwhich may introduce noise and make the reasoning less\\nreliable. This also highlights the importance of using general LLMs to conduct inductive reason-\\ning over multiple paths to disregard the noise and find the correct answers. Although the graph-\\nconstrained decoding benefits from the parallel GPU computation to explore multiple reasoning\\npaths at the same time, the time cost still slightly increases from 1.4s to 7.8s with the increase of\\nthe beam size. Thus, we set the beam size to 10 in the experiments to balance the performance and\\nefficiency. We also investigate the impact of L hops paths used for KG-Trie construction in Sec-\\ntion 10.1. The results show that GCR can achieve a good balance between reasoning performance\\nand efficiency by setting L = 2 and K = 10.\\n5.3\\nRQ2: HALLUCINATION ELIMINATION AND FAITHFUL REASONING\\nIn this section, we investigate the effectiveness of KG constraints in eliminating hallucinations and\\nensuring faithful reasoning. We first compare the difference of answer accuracy (Hit) and faithful\\nreasoning ratio by removing KG constraints in graph-constrained decoding. The faithful reasoning\\nratio is calculated as the percentage of faithful reasoning in correctly predicted answers. We define\\na reasoning as faithful where the generated reasoning path can be found in KGs, and vice versa.\\nGCR GCR w/o constraint\\n0\\n20\\n40\\n60\\nAnswer Hit\\n100.0%\\n62.4%\\nWebQSP\\nFaithful Reasoning\\nError Reasoning\\nGCR GCR w/o constraint\\n0\\n20\\n40\\n60\\nAnswer Hit\\n100.0%\\n48.1%\\nCWQ\\nFigure 5: Analysis of performance and reasoning\\nerrors in GCR.\\nFrom the Figure 5, we can observe that GCR\\nachieves the 100% faithful reasoning ratio on\\nboth datasets, which indicates that GCR can\\neliminate hallucinations and ensure faithful rea-\\nsoning during reasoning on KGs. In contrast,\\nwhen removing KG constraints, both the an-\\nswer accuracy and faithful reasoning decrease\\nsignificantly on WebQSP. This shows that KG\\nconstraints not only improve reasoning by re-\\nducing the searching space, but also play a cru-\\ncial role in preventing hallucinations for accu-\\n9\\nTable 5: Examples of the faithful reasoning conducted by GCR. Red denotes the incorrect reasoning\\npaths and answers, while bold denotes the correct paths and answers.\\nCase 1: Incorrect answers and hallucinated reasoning paths without constraints.\\nQuestion\\nWho is niall ferguson ’s wife?\\nAnswer\\nAyaan Hirsi Ali\\nGCR w/o constraint\\n# Reasoning Path: Niall Ferguson →people.person.children →Mabel Rose Ferguson →\\npeople.person.parents →Alyssa Mastromonaco\\n#Answer: Alyssa Mastromonaco\\nGCR\\n# Reasoning Path: Niall Ferguson →people.person.children →Thomas Ferguson →peo-\\nple.person.parents →Ayaan Hirsi Ali\\n#Answer: Ayaan Hirsi Ali\\nCase 2: Correct answers but hallucinated reasoning paths without constraints.\\nQuestion\\nWhere is jamarcus russell from?\\nAnswer\\nMobile\\nGCR w/o constraint\\n# Reasoning Path: JaMarcus Russell →people.person.place of birth →Tampa\\n#Answer: Mobile, Alabama\\nGCR\\n# Reasoning Path: JaMarcus Russell →people.person.place of birth →Mobile\\n#Answer: Mobile\\nrate reasoning. While the answer hit rate on CWQ remains almost unchanged, the ratio of faithful\\nreasoning still decreases to 48.1%. This implies that even if LLMs can produce correct answers, the\\nreasoning process is still prone to hallucinations and cannot be trusted, which is aligned with the\\nfindings in previous studies (Nguyen et al., 2024).\\nCase Study. We further provide a case study to illustrate the effectiveness of GCR in eliminating\\nhallucinations and ensuring faithful reasoning. As shown in Table 5, the first case demonstrates that,\\nwithout constraints, the model generates an incorrect reasoning path leading to an incorrect answer\\nby hallucinating facts such as “Mabel Rose Ferguson is the child of Naill Ferguson and her parent\\nis Alyssa Mastromonaco”. In contrast, GCR generates a faithful reasoning path grounded in KGs\\nthat “Naill Ferguson has a child named Thomas Ferguson who has a parent named Ayaan Hirsi Ali”.\\nBased on the paths we can reason the correct answer to the question is “Ayaan Hirsi Ali”. In the\\nsecond case, although the LLM answers the question correctly, the generated reasoning path is still\\nhallucinated with incorrect facts. Conversely, GCR conducts faithful reasoning with both correct\\nanswer and reasoning path. These results demonstrate that GCR can effectively eliminate hallucina-\\ntions and ensure faithful reasoning by leveraging KG constraints in graph-constrained decoding.\\n5.4\\nRQ3: ZERO-SHOT GENERALIZABILITY TO UNSEEN KGS\\nTable 6: Zero-shot transferabil-\\nity to other KGQA datasets.\\nModel\\nCSQA\\nMedQA\\nChatGPT\\n79\\n64\\nGCR (ChatGPT)\\n85\\n66\\nGPT-4o-mini\\n91\\n75\\nGCR (GPT-4o-mini)\\n94\\n79\\nIn GCR, the knowledge graph is converted into a constraint which\\nis plugged into the decoding process of LLMs. This allows GCR\\nto generalize to unseen KGs without further training. To evaluate\\nthe generalizability of GCR, we conduct zero-shot transfer ex-\\nperiments on two unseen KGQA datasets: CSQA (Talmor et al.,\\n2019) and MedQA (Jin et al., 2021). Specifically, we use the\\nsame KG-specialized LLM (Llama-3.1-8B) trained on Freebase\\nas well as two general LLMs (ChatGP, GPT-4o-mini). During\\nreasoning, we directly plug the KG-Trie constructed from ConceptNet and medical KGs into the\\nGCR to conduct graph-constrained decoding without additional fine-tuning. The results are shown\\nin Table 6.\\nFrom the results, it is evident that GCR outperforms ChatGPT and GPT-4o-mini in zero-shot per-\\nformance on both datasets. Specifically, GCR shows a 7.6% increase in accuracy on CSQA and a\\n3.1% improvement on MedQA compared to ChatGPT. This highlights the strong zero-shot general-\\nizability of its graph reasoning capabilities to unseen KGs without additional training. However, the\\nimprovement on MedQA is not as significant as that on CSQA. We hypothesize this difference may\\nbe due to LLMs having more common sense knowledge, which aids in reasoning on common sense\\nknowledge graphs effectively. On the other hand, medical KGs are more specialized and require\\ndomain-specific knowledge for reasoning, potentially limiting the generalizability of our method.\\n10\\n6\\nCONCLUSION\\nIn this paper, we introduce a novel LLM reasoning paradigm called graph-constrained reasoning\\n(GCR) to eliminate hallucination and ensure faithful reasoning by incorporating structured KGs. To\\nbridge the unstructured reasoning in LLMs with the structured knowledge in KGs, we propose a\\nKG-Trie to encode paths in KGs using a trie-based index. KG-Trie constrains the decoding process\\nto guide a KG-specialized LLM to generate faithful reasoning paths grounded in KGs. By impos-\\ning constraints, we can not only eliminate hallucination in reasoning but also reduce the reasoning\\ncomplexity, contributing to more efficient and accurate reasoning. Last, a powerful general LLM is\\nutilized as a complement to inductively reason over multiple reasoning paths to generate the final\\nanswer. Extensive experiments demonstrate that GCR excels in faithful reasoning and generalizes\\nwell to reason on new KGs without additional fine-tuning.\\nACKNOWLEDGMENTS\\nWe would want to express our sincere gratitude to Yuan-Fang Li for his valuable feedback and\\nsuggestions during the preparation of this work.\\nREFERENCES\\nGarima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, and Huan Liu. Mindful-rag: A study of\\npoints of failure in retrieval augmented generation. arXiv preprint arXiv:2407.12216, 2024.\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collab-\\noratively created graph database for structuring human knowledge. In Proceedings of the 2008\\nACM SIGMOD international conference on Management of data, pp. 1247–1250, 2008.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.\\nChen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. Knowledge is flat: A seq2seq generative\\nframework for various knowledge graph completion. In Proceedings of the 29th International\\nConference on Computational Linguistics, pp. 4005–4017, 2022.\\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval.\\nIn International Conference on Learning Representations, 2022.\\nMohammad Dehghan, Mohammad Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi,\\nAbbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen,\\nPrasanna Parthasarathi, Mahdi Biparva, and Mehdi Rezagholizadeh. EWEK-QA : Enhanced web\\nand efficient knowledge graph retrieval for citation-based question answering systems. In Lun-\\nWei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14169–14187,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.764.\\nZixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, and Xin Zhou. Ef-\\nfiqa: Efficient question-answering with strategic multi-model collaboration on knowledge graphs.\\narXiv preprint arXiv:2406.01238, 2024.\\nJonathan St BT Evans. Intuition and reasoning: A dual-process perspective. Psychological Inquiry,\\n21(4):313–326, 2010.\\nMarcello Federico, Mauro Cettolo, Fabio Brugnara, and Giuliano Antoniol. Language modelling\\nfor efficient beam-search. Computer Speech and Language, 9(4):353–380, 1995.\\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-\\nhop relational reasoning for knowledge-aware question answering. In Proceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1295–1309,\\n2020.\\n11\\nEdward Fredkin. Trie memory. Communications of the ACM, 3(9):490–499, 1960.\\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. Improving multi-hop knowl-\\nedge base question answering by learning intermediate supervision signals. In Proceedings of the\\n14th ACM international conference on web search and data mining, pp. 553–561, 2021.\\nMatthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi,\\nPavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A Saurous. Training chain-of-thought\\nvia latent-variable inference. Advances in Neural Information Processing Systems, 36, 2024.\\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.\\nIn Findings of the Association for Computational Linguistics: ACL 2023, pp. 1049–1065, 2023.\\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song,\\nand Denny Zhou.\\nLarge language models cannot self-correct reasoning yet.\\nIn The Twelfth\\nInternational Conference on Learning Representations, 2024.\\nGautier Izacard and ´Edouard Grave. Leveraging passage retrieval with generative models for open\\ndomain question answering. In Proceedings of the 16th Conference of the European Chapter of\\nthe Association for Computational Linguistics: Main Volume, pp. 874–880, 2021.\\nJinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen. Unikgqa: Unified retrieval and reasoning\\nfor solving multi-hop question answering over knowledge graph. In The Eleventh International\\nConference on Learning Representations, 2022.\\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: A\\ngeneral framework for large language model to reason over structured data. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural Language Processing, pp. 9237–9251, 2023.\\nJinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong\\nWen. Kg-agent: An efficient autonomous agent framework for complex reasoning over knowl-\\nedge graph. arXiv preprint arXiv:2402.11163, 2024.\\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-\\nease does this patient have? a large-scale open domain question answering dataset from medical\\nexams. Applied Sciences, 11(14):6421, 2021.\\nShiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing\\nYin. Graph reasoning for question answering with triplet retrieval. In Findings of the Association\\nfor Computational Linguistics: ACL 2023, pp. 3366–3375, 2023.\\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and\\ninterpretable large language model reasoning. In International Conference on Learning Repre-\\nsentations, 2024.\\nCostas Mavromatis and George Karypis. Rearev: Adaptive reasoning for question answering over\\nknowledge graphs. In Findings of the Association for Computational Linguistics: EMNLP 2022,\\npp. 2447–2458, 2022.\\nCostas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139, 2024.\\nMeta. Build the future of ai with meta llama 3, 2024. URL https://llama.meta.com/\\nllama3/.\\nThi Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, and Gho-\\nlamreza Haffari. Direct evaluation of chain-of-thought in multi-hop reasoning with knowledge\\ngraphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association\\nfor Computational Linguistics ACL 2024, pp. 2862–2883, Bangkok, Thailand and virtual meeting,\\nAugust 2024. Association for Computational Linguistics. URL https://aclanthology.\\norg/2024.findings-acl.168.\\nOpenAI. Introducing chatgpt, 2022. URL https://openai.com/index/chatgpt/.\\n12\\nOpenAI. Hello gpt-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\\nOpenAI.\\nNew embedding models and api updates, 2024b.\\nURL https://openai.com/\\nindex/new-embedding-models-and-api-updates/.\\nOpenAI.\\nLearning to reason with llms, 2024c.\\nURL https://openai.com/index/\\nlearning-to-reason-with-llms/.\\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large\\nlanguage models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data\\nEngineering (TKDE), 2024.\\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei\\nHuang, and Huajun Chen. Reasoning with language model prompting: A survey. In Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), pp. 5368–5393, 2023.\\nNils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese bert-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing. Association for Computational Linguistics, 11 2019.\\nURL https://arxiv.\\norg/abs/1908.10084.\\nDevendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training\\nof multi-document reader and retriever for open-domain question answering. Advances in Neural\\nInformation Processing Systems, 34:25968–25981, 2021.\\nRobyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of\\ngeneral knowledge. In Proceedings of the AAAI conference on artificial intelligence, volume 31,\\n2017.\\nKE Stanovich, RF West, and R Hertwig. Individual differences in reasoning: Implications for the ra-\\ntionality debate?-open peer commentary-the questionable utility of cognitive ability in explaining\\ncognitive illusions. 2000.\\nYuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang, and Bryan Hooi.\\nFidelis: Faithful\\nreasoning in large language model for knowledge graph question answering.\\narXiv preprint\\narXiv:2405.13873, 2024.\\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and\\nWilliam Cohen. Open domain question answering using early fusion of knowledge bases and\\ntext. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-\\ncessing, pp. 4231–4242, 2018.\\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni,\\nHeung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large\\nlanguage model on knowledge graph. In The Twelfth International Conference on Learning Rep-\\nresentations, 2024.\\nAlon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions.\\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 641–\\n651, 2018.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n13\\nKeheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong,\\nand Zhang Xiong. Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-\\nintensive question answering. arXiv preprint arXiv:2308.13259, 2023.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\\nmodels. In The Eleventh International Conference on Learning Representations, 2024.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\\nNeural Information Processing Systems, 35:24824–24837, 2022.\\nWikipedia contributors. Trie. https://en.wikipedia.org/wiki/Trie, 2024. Accessed:\\n2024-09-11.\\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\\ncomprehensive survey on graph neural networks. IEEE transactions on neural networks and\\nlearning systems, 32(1):4–24, 2020.\\nXin Xie, Ningyu Zhang, Zhoubo Li, Shumin Deng, Hui Chen, Feiyu Xiong, Mosha Chen, and\\nHuajun Chen. From discrimination to generation: Knowledge graph completion with generative\\ntransformer. In Companion Proceedings of the Web Conference 2022, pp. 162–165, 2022.\\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai,\\nJinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng\\nXue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai\\nBai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan\\nZhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang\\nZhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2\\ntechnical report. arXiv preprint arXiv:2407.10671, 2024a.\\nRui Yang, Haoran Liu, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James\\nCaverlee, Yutaka Matsuo, and Irene Li. Kg-rank: Enhancing large language models for medical\\nqa with knowledge graphs and ranking techniques. arXiv preprint arXiv:2403.05881, 2024b.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\\nvances in Neural Information Processing Systems, 36, 2024.\\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn:\\nReasoning with language models and knowledge graphs for question answering. In Proceedings\\nof the 2021 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, pp. 535–546, 2021.\\nWen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value\\nof semantic parse labeling for knowledge base question answering. In Proceedings of the 54th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp.\\n201–206, 2016.\\nPing Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi\\nGhosh, Mona Diab, and Asli Celikyilmaz. Alert: Adapting language models to reasoning tasks.\\narXiv preprint arXiv:2212.08286, 2022.\\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph\\nretrieval enhanced model for multi-hop knowledge base question answering. In Proceedings of the\\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npp. 5773–5784, 2022.\\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\\naugment large language models. arXiv preprint arXiv:2310.07554, 2023.\\n14\\nYuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang,\\nJinjie Gu, and Huajun Chen. Knowagent: Knowledge-augmented planning for llm-based agents.\\narXiv preprint arXiv:2403.03101, 2024.\\nAppendix\\nTable of Contents\\n7\\nDatasets\\n15\\n8\\nBaselines\\n16\\n9\\nImplementation Details and Experiment Settings\\n17\\n10 Additional Experiment Results\\n19\\n10.1 Performance on Different Hops . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n11 Templates and Prompts\\n19\\n7\\nDATASETS\\nKGQA Datasets. To compare the reasoning performance with existing methods, we use two bench-\\nmark KGQA datasets in this study: WebQuestionSP (WebQSP) (Yih et al., 2016) and Complex We-\\nbQuestions (CWQ) (Talmor & Berant, 2018). To ensure fairness, we adopt the same train and test\\nsplits as previous works (Jiang et al., 2022; Luo et al., 2024). Details of the datasets can be found in\\nTable 7.\\nBoth WebQSP and CWQ can be reasoned using Freebase KGs2 (Bollacker et al., 2008). To reduce\\nthe size of the KGs, we use a subgraph of Freebase by extracting all triples that start from question\\nentities within the maximum reasoning hops provided by previous works3 (Luo et al., 2024). The\\nstatistics of the knowledge graphs are shown in Table 9.\\nFine-tuning Datasets. To enhance the KG reasoning ability of LLMs, we construct fine-tuning\\ndatasets by generating reasoning paths from the KGs. Specifically, we adopt the training split of\\nWebQSP and CWQ, which contain 2,826 and 27,639 question-answer pairs, respectively. For each\\nquestion, we find all the shortest reasoning paths on KGs that connect the question entity to the\\nanswer entity. We then convert the reasoning paths into formatted strings and pair them with the\\nquestion-answer pairs with the template shown in Figure 8 to form the fine-tuning datasets. Since\\nthere could be multiple reasoning paths for a question, we generate multiple training instances paired\\nwith different reasoning paths for each question-answer pair. The fine-tuning datasets contain 28,307\\nand 181,602 question-reasoning path-answer triples for WebQSP and CWQ, respectively. The statis-\\ntics of the fine-tuning datasets are shown in Table 8.\\nZero-shot Generalization Datasets.\\nTo evaluate the transferability of GCR, we further select\\ntwo new KGQA datasets4: CommonsenseQA (CSQA) (Talmor et al., 2019) and MedQA-USMLE\\n(MedQA) (Jin et al., 2021). CSQA is a 5-way multiple choice QA dataset that involves reasoning\\nwith commonsense knowledge. MedQA is a 4-way multiple choice QA task that requires biomed-\\nical and clinical knowledge. For CSQA, we use the ConceptNet (Speer et al., 2017), which is a\\ngeneral-purpose KG that contains commonsense knowledge. For MedQA, we use a medical KG\\n2https://github.com/microsoft/FastRDFStore\\n3WebQSP: https://huggingface.co/datasets/rmanluo/RoG-webqsp, CWQ: https://\\nhuggingface.co/datasets/rmanluo/RoG-cwq\\n4https://github.com/michiyasunaga/qagnn\\n15\\nconstructed from the Unified Medical Language System (Yasunaga et al., 2021). The statistics of\\nthe knowledge graphs are shown in Table 9. We respectively select 100 questions from each dataset.\\nFor each question, following previous studies (Feng et al., 2020; Yasunaga et al., 2021), a 2-hop\\nsubgraph is extracted from the KGs to form the zero-shot generalization datasets.\\nTable 7: Statistics of datasets.\\nDataset\\nDataset Statistics\\nStatistics of Answer Numbers\\n#Train\\n#Test\\n#Ans = 1\\n2 ≥#Ans ≤4\\n5 ≥#Ans ≤9\\n#Ans ≥10\\nWebQSP\\n2,826\\n1,628\\n51.2%\\n27.4%\\n8.3%\\n12.1%\\nCWQ\\n27,639\\n3,531\\n70.6%\\n19.4%\\n6%\\n4%\\nTable 8: Statistics of fine-tuning datasets for graph-constrained decoding.\\nTotal\\nWebQSP\\nCWQ\\n209,909\\n28,307\\n181,602\\nTable 9: Statistics of constructed knowledge graphs.\\nKG\\n#Entities\\n#Relations\\n#Triples\\nFreebase\\n2,566,291\\n7,058\\n8,309,195\\nConceptNet\\n799,273\\n17\\n2,151,303\\nMedKG\\n9,958\\n15\\n49,974\\n8\\nBASELINES\\nWe compare GCR with the 22 baselines grouped into three categories: 1) LLM reasoning methods,\\n2) graph reasoning methods, and 3) KG-enhanced LLM reasoning methods. The details of each\\nbaseline are described as follows.\\nLLM reasoning methods only rely on LLMs for reasoning without utilizing external KGs. We\\ninclude both the vanilla LLMs with different sizes and the LLMs with advanced reasoning mecha-\\nnisms. Specifically, we consider the following baselines:\\n• Qwen2-0.5B/1.5B.7B (Yang et al., 2024a) provides a series of pre-trained LLMs with dif-\\nferent sizes, including 0.5B, 1.5B, and 7B parameters.\\n• Llama-2-7B (Touvron et al., 2023) is a large-scale LLM pre-trained on a diverse range of\\ntasks.\\n• Llama-3.1-8B (Meta, 2024) is the updated version of Llama-2 with more powerful reason-\\ning capabilities.\\n• ChatGPT (OpenAI, 2022) is a powerful closed-source LLM that could follow instructions\\nto conduct complex tasks.\\n• GPT-4o-mini (OpenAI, 2024a) is the new flagship model of OpenAI that could reason\\nacross different modalities and tasks.\\n• Few-shot prompt (Brown et al., 2020) is a few-shot learning method that provides LLMs\\nwith a few examples in the prompts to conduct reasoning.\\n• CoT (Wei et al., 2022) is a chain-of-thought reasoning method that prompts LLMs to gen-\\nerate a chain of reasoning steps.\\n• Self-consistency (Wang et al., 2024) generates multiple reasoning paths and selects the\\nmost consistent answer.\\n16\\nGraph reasoning methods focus on reasoning on KGs using graph neural networks (GNNs) (Wu\\net al., 2020) or graph-based reasoning mechanisms. We include the following baselines:\\n• GraftNet (Sun et al., 2018) is a graph-based reasoning method that retrieves relevant sub-\\ngraphs from KGs with entity linking.\\n• NSM (He et al., 2021) utilizes the sequential model to mimic the multi-hop reasoning\\nprocess on KGs.\\n• SR+NSM (Zhang et al., 2022) proposes a relation-path retrieval to retrieve subgraphs for\\nmulti-hop reasoning.\\n• ReaRev (Mavromatis & Karypis, 2022) is a GNN-based method that reasons on KGs by\\nconsidering complex graph information.\\nKG-enhanced LLM reasoning methods incorporate KGs to enhance the reasoning abilities of\\nLLMs which can be further divided into retrieval-based and agent-based paradigms. We include the\\nfollowing baselines:\\nRetrieval-based methods retrieve relevant facts from KGs with an external retriever and then feed\\nthem into the inputs of LLMs for reasoning:\\n• KD-CoT (Wang et al., 2023) retrieves relevant knowledge from KGs to generate faithful\\nreasoning plans for LLMs.\\n• EWEK-QA (Dehghan et al., 2024) enriches the retrieved knowledge by searching from\\nboth KGs and web.\\n• RoG (Luo et al., 2024) proposes a planning-retrieval-reasoning framework that retrieves\\nreasoning paths from KGs to guide LLMs conducting faithful reasoning.\\n• GNN-RAG (Mavromatis & Karypis, 2024) adopts a lightweight graph neural network to\\neffectively retrieve from KGs.\\n• GNN-RAG+RA (Mavromatis & Karypis, 2024) combines the retrieval results of both RoG\\nand GNN-RAG to enhance the reasoning performance.\\nAgent-based methods treat LLMs as agents that iteratively interact with KGs to find reasoning paths\\nand answers:\\n• ToG (Sun et al., 2024) conducts the reasoning on KGs by exploring multiple paths and\\nconcludes the final answer by aggregating the evidence from them.\\n• EffiQA (Jiang et al., 2024) proposes an efficient agent-based method to reason on KGs.\\n9\\nIMPLEMENTATION DETAILS AND EXPERIMENT SETTINGS\\nIn this section, we will detail the implementation of GCR as well as the experiment settings.\\nFine-tuning KG-specialized LLMs. We fine-tune several lightweight LLMs ranging from 0.5B to\\n8B (Yang et al., 2024a; Touvron et al., 2023; Meta, 2024) on the fine-tuning datasets for 3 epochs.\\nThe batch size is set to 4 and the learning rate is set to 2e-5. We use the cosine learning rate scheduler\\npolicy with the warmup ratio set to 0.03. The training is conducted on 2 A100-80G GPUs for each\\nmodel. The training time and memory usage are shown in Table 10.\\nKGQA Experiment Settings. The KGQA experiment shown in Table 1 aims to compare the rea-\\nsoning performance of GCR with existing methods. For our method, we use the fine-tuned Llama-\\n3.1-8B as KG-specialized LLMs, the general LLM is selected as ChatGPT and GPT-4o-mini. The\\nKG-Trie is constructed from the subgraph of Freebase KGs. The maximum reasoning hops are set\\nto 2 for both WebQSP and CWQ. The beam size is set to 10 for graph-constrained decoding. For\\nvanilla LLMs baselines, we use the zero-shot prompting to ask the models to answer the questions.\\nFor other baselines, we strictly check whether the original papers follow the same settings and copy\\nthe results for fair comparison.\\nEfficiency Analysis Settings. The efficiency analysis shown in Table 2 aims to compare the effi-\\nciency and performance of different methods on WebQSP. For GCR, we use the same settings as the\\n17\\nTable 10: Training time and memory usage for different KG-specialized LLMs.\\nModel\\nTime\\nMem. Usage per GPU\\nQwen2-0.5B\\n3.47h\\n10G\\nQwen2-1.5B\\n4.11h\\n25G\\nQwen2-7B\\n14.37h\\n81G\\nLlama-2-7B\\n13.93h\\n80G\\nLlama-3.1-8B\\n14.52h\\n85G\\nKGQA experiment. For dense retriever methods (e.g., S-Bert (Reimers & Gurevych, 2019), BGE\\n(Zhang et al., 2023), OpenAI-Emb. (OpenAI, 2024b)), we first search all paths within 2-hops on the\\nKGs which are formatted as sentences with the template in Figure 7. Then, we adopt the embedding\\nmodel to encode the path sentences as embeddings which are stored in a vector database. During in-\\nference, we retrieve 10 paths from the vector database with the question as query and feed them into\\nthe LLMs for reasoning. For GNN-RAG (Mavromatis & Karypis, 2024) and RoG (Luo et al., 2024),\\nwe strictly follow the original papers to retrieve reasoning paths and conduct the experiments. For\\nagent-based methods (e.g., ToG (Sun et al., 2024)), we use the same settings detailed in the original\\npapers. For EffiQA (Jiang et al., 2024), since there is no available code, we directly copy the results\\nfrom the original paper.\\nThe average runtime is measured by the time taken to answer the questions. The average number\\nof LLM calls is the number of times the LLMs are called to answer the questions. The average\\nnumber of LLM tokens is the number of tokens inputted into LLMs to answer the questions, such\\nas questions and retrieved reasoning paths. The experiments are conducted on a single A100-80G\\nGPU for each method.\\nAblation Study. In ablation study, we first try to analyze the effectiveness of different components in\\nGCR. We conduct the experiments on WebQSP and CWQ datasets. By removing the KG-specialized\\nLLM (w/o KG-specialized LLM), we search all the 2-hop paths starting from question entities and\\nfeed them into the general LLMs for reasoning. By removing the general LLM (w/o general LLM),\\nwe directly use the hypothesis answers generated by the KG-specialized LLMs as the final answers.\\nDifferent LLMs. We also analyze the different LLMs used for KG-specialized LLMs and general\\nLLMs on WebQSP. For KG-specialized LLMs, we first use the vanilla LLMs with different learning\\ntypes (i.e., zero-shot and few-shot prompting). For zero-shot prompting, we directly ask the models\\nto generate the reasoning paths with the constraints. For few-shot prompting, we provide the models\\nwith a few examples in the prompts to conduct path generation. Detailed prompts can be found in\\nFigures 8 and 10. Then, we fine-tune the lightweight LLMs with different sizes (0.5B to 8B) on the\\ngraph-constrained decoding task. For general LLMs, we use the vanilla LLMs to directly conduct\\nreasoning over multiple reasoning paths. The detailed reasoning prompts can be found in Figure 9.\\nParameter Analysis. We first analyze the performance of GCR with different beam sizes for graph-\\nconstrained decoding. We conduct the experiments on the WebQSP datasets with beam sizes of 1, 3,\\n5, 10, and 20. Then, we analyze the performance of GCR with different hops of paths encoded in the\\nKG-Trie. We conduct the experiments on the WebQSP datasets with maximum paths hops ranging\\nfrom 1 to 4.\\nFaithful Reasoning Analysis. We investigate the effect of the KG constraints on ensuring faithful\\nreasoning. We adopt the fine-tuned Llama-3.1-8B as KG-specialized LLMs. Then, we compare\\nthe faithful reasoning rate and answer hit of GCR with and without the KG constraints in graph-\\nconstrained decoding. The faithful reasoning rate is the percentage of the faithful reasoning in the\\ncorrectly predicted answers. A reasoning path is considered faithful if it can be found in the KGs,\\nand vice versa. The answer hit is the percentage of the correct answers in the predictions.\\nZero-shot Generalization Analysis. We evaluate the transferability of GCR on two zero-shot gen-\\neralization datasets: CSQA and MedQA. We use the fine-tuned Llama-3.1-8B as KG-specialized\\nLLMs and ChatGPT as well as GPT-4o-mini as the general LLMs. The KG-Trie is constructed\\nfrom the subgraph of ConceptNet and MedKG. The maximum reasoning hops are set to 2 for both\\ndatasets. The beam size is set to 10 for graph-constrained decoding. For vanilla LLMs baselines\\n18\\n(i.e., ChatGPT and GPT-4o-mini), we use the zero-shot prompting to ask the models to answer the\\nquestions.\\n10\\nADDITIONAL EXPERIMENT RESULTS\\n10.1\\nPERFORMANCE ON DIFFERENT HOPS\\nIn this section, we analyze the impact of different hops of reasoning paths on the performance of\\nGCR. We conduct the experiments on WebQSP with different maximum hops of reasoning paths\\nencoded in the KG-Trie. The results are shown in Figure 6. We observe that the performance\\nof GCR increases with the number of hops of reasoning paths. The performance peaks when the\\nmaximum hops of reasoning paths are set to 2. This is because the 2-hop paths can provide sufficient\\ninformation for the LLMs to conduct reasoning. When the hops are set to 3 or 4, the performance\\ndrops due to the increased complexity of the reasoning paths, which may introduce noise and make\\nthe reasoning less reliable. Additionally, the size of the KG-Trie slightly increases from 0.5 MB to\\n7.5 MB with the increase of the hops from 1 to 4. This indicates that the KG-Trie can be efficiently\\nconstructed with a small size and guide the LLMs to reason on graphs effectively.\\n1\\n2\\n3\\n4\\nKG-Trie Path Length L\\n0\\n2\\n4\\n6\\nAvg. KG-Trie size (MB)\\n60\\n70\\n80\\n90\\nAnswer Coverage (%)\\nAvg. KG-Trie size (MB)\\nHit\\nF1\\nPrecision\\nRecall\\nFigure 6: Parameter analysis of path hop L for KG-Trie construction on WebQSP.\\n11\\nTEMPLATES AND PROMPTS\\nIn this section, we illustrate all the templates and prompts used in the experiments.\\nPath Sentence Template. The template for converting reasoning paths into natural language sen-\\ntences is shown in Figure 7, where the e∗and r∗denotes the entities and relations in a reasoning\\npath wz = e0\\nr1\\n−→e1\\nr2\\n−→. . .\\nrl\\n−→el,\\nPath Sentence Template\\n<PATH> e1 →r1 →e2 →. . . →rl →el </PATH>\\nFigure 7: The template for converting reasoning paths into formatted sentences.\\nGraph-constrained Decoding Prompt. The prompt for graph-constrained decoding is shown in\\nFigure 8, where the question and mentioned entities are provided to the LLMs to generate rea-\\nsoning paths and hypothesis answers. In the fine-tuning datasets, the supervised LLM outputs are\\nconstructed from the ground-truth answers and reasoning paths extracted from the KGs.\\n19\\nGraph-constrained Decoding Prompt\\n============================= Prompt Input ================================\\nReasoning path is a sequence of triples in the KG that connects the topic entities in the question to\\nanswer entities. Given a question, please generate some reasoning paths in the KG starting from the\\ntopic entities to answer the question.\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n============================= LLM Output ================================\\n# Reasoning Path:\\n<PATH> <Reasoning Path> </PATH>\\n# Answer:\\n<Hypothesis Answer>\\nFigure 8: The prompt template for graph-constrained decoding.\\nThe few-shot prompt template for graph-constrained decoding is shown in Figure 10. We provide a\\nfew examples in the prompts to guide the LLMs to generate reasoning paths. Since the LLMs with\\nfew-shot prompt learning are not fine-tuned on the graph-constrained decoding task, we only apply\\nthe constraint to generate reasoning paths.\\nGraph Inductive Reasoning Prompt. The prompt for graph inductive reasoning is shown in Fig-\\nure 9. We adopt the graph-constrained decoding to generate K reasoning paths and hypothesis\\nanswers for each question. The reasoning paths and hypothesis answers are provided to the general\\nLLMs to answer the questions without fine-tuning.\\nGraph Inductive Reasoning Prompt\\n============================= Prompt Input ================================\\n# Reasoning Paths:\\n<Reasoning Path 1><Hypothesis Answer 1>\\n. . .\\n<Reasoning Path K><Hypothesis Answer K>\\n# Question:\\n<Question>\\nBased on the reasoning paths, please answer the given question. Please keep the answer as simple as\\npossible and only return answers. Please return each answer in a new line.\\n============================= LLM Output ================================\\n<Answer 1>\\n<Answer 2>\\n. . .\\nFigure 9: The prompt template for graph inductive reasoning.\\n20\\nFew-shot Graph-constrained Decoding Prompt\\n============================= Prompt Input ================================\\nReasoning path is a sequence of triples in the KG that connects the topic entities in the question to\\nanswer entities. Given a question, please generate some reasoning paths in the KG starting from the\\ntopic entities to answer the question.\\nExample 1\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n# Reasoning Path:\\n<Reasoning Path>\\nExample 2\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n# Reasoning Path:\\n<Reasoning Path>\\nExample 3\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n# Reasoning Path:\\n<Reasoning Path>\\nInput\\n# Question:\\n<Question>\\n# Topic entities:\\n<Question Entities>\\n============================= LLM Output ================================\\n# Reasoning Path:\\n<Reasoning Path>\\nFigure 10: The few-shot prompt template for graph-constrained decoding.\\n21\\n'),\n",
       " Document(metadata={'Published': '2024-05-09', 'Title': 'Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models', 'Authors': 'Yitian Li, Jidong Tian, Hao He, Yaohui Jin', 'Summary': 'Combining different forms of prompts with pre-trained large language models\\nhas yielded remarkable results on reasoning tasks (e.g. Chain-of-Thought\\nprompting). However, along with testing on more complex reasoning, these\\nmethods also expose problems such as invalid reasoning and fictional reasoning\\npaths. In this paper, we develop \\\\textit{Hypothesis Testing Prompting}, which\\nadds conclusion assumptions, backward reasoning, and fact verification during\\nintermediate reasoning steps. \\\\textit{Hypothesis Testing prompting} involves\\nmultiple assumptions and reverses validation of conclusions leading to its\\nunique correct answer. Experiments on two challenging deductive reasoning\\ndatasets ProofWriter and RuleTaker show that hypothesis testing prompting not\\nonly significantly improves the effect, but also generates a more reasonable\\nand standardized reasoning process.'}, page_content='Hypothesis Testing Prompting Improves Deductive Reasoning in\\nLarge Language Models\\nYitian Li1,2, Jidong Tian1,2, Hao He1,2, Yaohui Jin1,2\\n1MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\n2State Key Lab of Advanced Optical Communication System and Network\\n{yitian_li, frank92, hehao, jinyh}@sjtu.edu.cn\\nAbstract\\nCombining different forms of prompts with pre-trained large language models has yielded remarkable results on\\nreasoning tasks (e.g. Chain-of-Thought prompting). However, along with testing on more complex reasoning, these\\nmethods also expose problems such as invalid reasoning and fictional reasoning paths. In this paper, we develop\\nHypothesis Testing Prompting, which adds conclusion assumptions, backward reasoning, and fact verification during\\nintermediate reasoning steps. Hypothesis Testing prompting involves multiple assumptions and reverses validation of\\nconclusions leading to its unique correct answer. Experiments on two challenging deductive reasoning datasets\\nProofWriter and RuleTaker show that hypothesis testing prompting not only significantly improves the effect, but also\\ngenerates a more reasonable and standardized reasoning process.\\nKeywords: Deductive Reasoning, Large Language Models, Prompt\\n1.\\nIntroduction\\nThe release of large language models (LLMs) has\\nrevolutionized the NLP landscape recently (Thop-\\npilan et al., 2022; Kaplan et al., 2020; Chowdh-\\nery et al., 2022). Scaling up the size of language\\nmodels and conducting diversified prompt meth-\\nods become mainstream (Liu et al., 2023c; Wei\\net al., 2022a; Yang et al., 2023). Given In-context\\nlearning or Chain-of-Thought prompts have already\\nachieved high performance on challenging tasks\\nsuch as commonsense, arithmetic, and symbolic\\nreasoning (Imani et al., 2023; Lee et al., 2021;\\nKojima et al., 2022). Logical reasoning is one of\\nthe most important and long-standing problems in\\nNLP (Hirschberg and Manning, 2015; Russell and\\nNorvig, 2010), and integrating this ability into nat-\\nural language understanding systems has always\\nbeen a goal pursued (Du et al., 2022).\\nNevertheless, scaling has been demonstrated\\nto offer limited advantages in resolving complex\\nlogical reasoning issues (Kazemi et al., 2022). For\\nexample, Saparov and He (2022) show that Chain-\\nof-Thought prompting struggles with proof planning\\nfor more complex logical reasoning problems. Addi-\\ntionally, the performance suffers greatly while han-\\ndling recently released and out-of-distribution logi-\\ncal reasoning datasets (Liu et al., 2023a). Despite\\nmany works have explored variants of Chain-of-\\nThought prompts to facilitate LLMs inference (Zelik-\\nman et al., 2022; Zheng et al., 2023), we discover\\nthat the present logical reasoning task prompts\\nplace an excessive amount of emphasis on the\\nreasoning process while ignoring the origin, pur-\\npose, and effectiveness of reasoning (Creswell\\net al., 2022; Xi et al., 2023). As examples shown in\\nQ1: Bob is green. True/false? \\nInput Facts: Alan is blue. Alan is \\nrough. Alan is young. Bob is big. \\nBob is round. Charlie is big. Charlie \\nis blue. Charlie is green. Dave is \\ngreen. Dave is rough.\\nInput Rules: Big people are rough. \\nIf someone is young and round then \\nthey are kind. If someone is round \\nand big then they are blue. All\\nrough people are green.\\nBob is big. \\nBig people are rough. \\nAll rough people are green.\\nAnswer: T\\nQ2: Dave is blue. True/false?\\nLess inspiring\\nFigure 1: Questions in RuleTaker involve logical\\nreasoning with facts and rules.\\nFigure 1, the difficulty in judging logical problems\\narises not only from the process of reasoning but\\nalso from the choice of facts and rules to use as a\\nstarting point. Even if we were provided the thought\\nprocess for some of the issues, it would not be very\\nbeneficial for others, based on how we previously\\ncreated the prompts.\\nIn this paper, we propose Hypothesis Testing\\nPrompting, a new and more considerate prompt\\ntemplate design idea. Hypothesis testing is a for-\\nmal procedure for investigating our ideas about\\nthe world using statistics and is often used by sci-\\nentists to test specific predictions (Bevans, 2022).\\nWe draw inspiration from its process to introduce a\\nprocess of conclusion assumptions, backward rea-\\nsoning, and fact verification. Experiments on Rule-\\nTaker (Clark et al., 2020) and ProofWriter (Tafjord\\net al., 2021) show the effectiveness of our novel\\nprompting paradigm as a strategy for promoting\\ndeductive reasoning in large language models. Fur-\\nther analyses show that Hypothesis Test prompting\\ngenerates more desirable intermediate processes\\narXiv:2405.06707v1  [cs.CL]  9 May 2024\\nand significantly improves the \"Unknown\" label.\\n2.\\nRelated Work\\n2.1.\\nFew-Shot Prompting\\nBrown et al. (2020) propose in-context learning as\\nan alternative few-shot prompting way to stimulate\\nability. Besides, chain-of-Thought (CoT) (Wei et al.,\\n2022b) is one of the most well-known works, which\\ndecomposes the problem into intermediate steps\\nand further improves the ability of large language\\nmodels. Subsequently, several follow-up works\\nwere carried out, including Zero-shot-CoT (simply\\nadding \"Let’s think step by step\" before each an-\\nswer) (Kojima et al., 2022), Self-consistency (Wang\\net al., 2022), complexity-based (Fu et al., 2022),\\nand other prompting work (Liu et al., 2023b; Jung\\net al., 2022; Zhou et al., 2022; Saparov and He,\\n2022). While these methods enhance the perfor-\\nmance of inference by paying attention to indica-\\ntions of the reasoning process, they often overlook\\nsome aspects such as identifying the root cause of\\nthe problem, establishing efficient reasoning strate-\\ngies, and determining the direction of logical rea-\\nsoning.\\n2.2.\\nDeductive Reasoning\\nDeductive reasoning is defined as the applica-\\ntion of general concepts to particular circum-\\nstances (Johnson-Laird, 2010). Making logical as-\\nsumptions is the foundation of deductive reasoning,\\nwhich then bases a conclusion on those assump-\\ntions. The deduction task is then applied to a sit-\\nuation from the actual world after starting with a\\nrule. In light of the principles \"All men are mortal.\"\\nand \"Socrates is a man.\" for example, we can draw\\nthe conclusion that \"Socrates is mortal.\" (Johnson-\\nLaird, 1999).\\n3.\\nHypothesis Testing Prompting\\nHypothesis testing is a formal procedure for investi-\\ngating our ideas about the world using statistics and\\nused by scientists to test specific predictions that\\narise from theories (Bevans, 2022; La et al., 2012).\\nThere are 5 main steps in hypothesis testing:\\n1. State your research hypothesis;\\n2. Collect data in a way designed to test the hy-\\npothesis;\\n3. Perform an appropriate statistical test;\\n4. Decide whether to reject or fail to reject your\\nnull hypothesis;\\n5. Present the findings in your results and discus-\\nsion section;\\nWhen completing a challenging reasoning activ-\\nity, such as a multi-step deductive reasoning prob-\\nlem, one is not conducting random reasoning to\\nobtain all possible intermediate results. We shall\\nchoose the relevant conditions for inference ver-\\nification after initially making assumptions about\\nthe judgment problem, such as \" First assume the\\nconclusion is True and start from ... Then assume\\nthe conclusion is False and start from ... because\\nthe rules state that ... So the conclusion ...\". The\\npurpose of this study is to give language models\\nthe capacity to build a process that is similar to\\nwhat we defined as Hypothesis Testing Prompt-\\ning. We will show that large language models can\\ngenerate more appropriate thought and more ac-\\ncurate results if demonstrations of hypothesis test\\nprompting are provided in the exemplars for few-\\nshot prompting. Figure 2 shows an example of a\\nmodel producing a hypothesis testing thought to\\nsolve a deductive reasoning problem.\\n4.\\nExperiment\\n4.1.\\nExperimental Setup\\nWe explore Hypothesis Test Prompting for Chat-\\nGPT (GPT-3.5-Turbo in the OpenAI API) on multiple\\nlogical reasoning benchmarks.\\nBenchmarks. Considering FOL reasoning in\\nquestion answering systems, there are two world\\nassumptions (Reiter, 1981) that result in different\\nobjectives. One is the closed world assumption\\n(CWA), which is the presumption that what is not\\ncurrently known to be entailment is contradiction.\\nThe other is the open world assumption (OWA),\\nwhose objective should distinguish false proposi-\\ntions from uncertain ones. Due to differences in\\nworld assumptions, our analysis and solutions are\\nalso different.\\nWe consider the following two deductive reason-\\ning problem benchmarks: (1) the RuleTaker (Clark\\net al., 2020) benchmark using CWA assumption;\\n(2) the ProofWriter (Tafjord et al., 2021) benchmark\\nusing OWA assumption. Both datasets are divided\\ninto five parts, each part requiring 0, ≤1, ≤2, ≤\\n3, and ≤5 hops of reasoning, respectively. We\\nconducted comparison tests on the test set of the\\ntwo datasets for 5 distinct hops.\\nStandard prompting. As one of the baselines,\\nwe take into account the common few-shot prompt-\\ning, made popular by Brown et al. (2020), in which\\na language model is provided with in-context ex-\\namples of input-output pairings before producing a\\nprediction for a test-time example. Examples are\\npresented in the form of questions and answers.\\nAs seen in Figure 2(above), the model directly an-\\nswers the question.\\nChain-of-Thought prompting. We also com-\\nChain-of-Thought Prompting\\nModel Input\\nStandard Prompting\\nModel Input\\nQ: Judge the following conclusion \\'Harry is cold.\\' is correct, wrong, or unknown, based on the following texts: Text 1\\nA: The conclusion \\'Harry is cold.\\' is True.\\nQ: Judge the following conclusion ‘Bob is not young.’ is correct, wrong, or unknown, based on the following texts: Text 2\\nModel Output\\nText 1: Charlie is rough. Charlie is white. Erin is blue. Erin is rough. \\nErin is young. Gary is blue. Gary is green. Gary is quiet. Gary is \\nwhite. Harry is blue. Harry is rough. All rough, white things are \\ngreen. All young, blue things are quiet. If something is young then it \\nis blue. White, blue things are quiet. If something is green then it is \\nyoung. Quiet, green things are cold. All green, cold things are rough.\\nQuestion: Harry is cold.     \\nAnswer: True\\nText 2: Bob is rough. Fiona is not furry. Fiona is white. Young, nice \\nthings are big. Blue things are big. If something is young and white \\nthen it is nice. All nice things are young. All white things are young. \\nIf something is blue and not big then it is not rough.\\nQuestion: Bob is not young. \\nAnswer: Unknown\\nQ: Judge the following conclusion \\'Harry is cold.\\' is correct, wrong, or unknown, based on the following texts: Text 1\\nA: Start from \\'Harry is quiet\\' and \\'Quiet things are nice\\'. Then can know \\'Harry is nice\\'. And \\'All nice things are cold\\' is given. Finally \\nit can be deduced \\'Harry is cold\\'. So the conclusion \\'Harry is cold.\\' is True.\\nQ: Judge the following conclusion \\'Harry is not quiet.\\' is correct, wrong, or unknown, based on the following texts: Text  2\\nModel Output\\nHypothesis Testing Prompting\\nModel Input\\nQ: Judge the following conclusion \\'Harry is cold.\\' is correct, wrong, or unknown, based on the following texts: Text 1\\nA: First assume the conclusion is True and start from Harry is cold. Because the rules state that all nice things are cold, correct\\nconclusion needs to be satisfied that Harry is nice. And the rules states that quiet things are nice. It needs to be satisfied Harry is quiet, \\nwhich is also stated in the facts. So the conclusion \\'Harry is cold.\\' is True.\\nQ: Judge the following conclusion ‘Bob is not young.’ is correct, wrong, or unknown, based on the following texts: Text  2\\nQ: The conclusion \\'Bob is not young.\\' is True.\\nQ: Start from \\'Bob is rough\\' and \\'If something is blue and not big then it is not rough\\'. Then can know \\'Bob is not blue\\'. And \\'All white \\nthings are young\\' is given. Finally it can be deduced \\'Bob is not young\\'. So the conclusion \\'Bob is not young.\\' is True.\\nModel Output\\nQ: First assume the conclusion is True and start from Bob is not young. Because the rules state that all white things are young, correct \\nconclusion needs to be satisfied that Bob is not white. And there are no facts or rules about \\'not white\\'. Then assume the conclusion is \\nFalse and start from Bob is young. Because the rules state that all nice things are young, correct conclusion needs to be satisfied that \\nBob is nice. And there are no facts or rules about \\'nice\\'. So the conclusion \\'Bob is not young.\\' is Unknown.\\n× N\\n× N\\n× N\\nFigure 2: Comparison of three prompting methods: (a) Standard (b) Chain-of-Thought (c) Hypothesis\\nTesting. Particularly, we highlight the Hypothesis testing reasoning processes. The comparative experi-\\nmental results show that: Hypothesis testing prompting enables large language models to tackle complex\\nlogical reasoning.\\npare with Chain-of-thought prompting which has\\nachieved encouraging results on complex reason-\\ning tasks (Wei et al., 2022b).\\nAs seen in Fig-\\nure 2(middle), the model not only provides the final\\nanswer but also comes with the consideration of\\nintermediate steps.\\nHypothesis Testing Prompting. Our proposed\\napproach is to augment each exemplars in few-shot\\nprompting with the thought of hypothesis testing for\\nan associated answer, as illustrated in Figure 2(be-\\nlow). We show one chain of thought exemplars\\n(Example: Judge the following conclusion ’<Con-\\nclusion>’ is true, false, or unknown, based on the\\nfollowing facts and rules: <Facts> ... <Rules> ...).\\n4.2.\\nExperimental Results\\nThe results for Hypothesis Testing Prompting and\\nthe baselines on the RuleTaker datasets are pro-\\nvided in Figure 3(a), and ProofWriter results are\\nshown in Figure 3(b). From the results, we ob-\\nserve that our method significantly outperforms the\\nother two baselines, especially on ProofWriter. Fig-\\nure 3(a) demonstrates that while CoT performs well\\nin the low hop, Hypothesis Testing prompting per-\\nforms better as the hops count increases on Rule-\\nTaker. While on ProofWriter, our approach has a\\nthorough lead (improved accuracy by over 4% on\\nall hops). Comparing two datasets, the latter dis-\\ntinguishes between \"False\" and \"Unknown\", which\\ndemand a greater level of logic. The results on two\\n0.97\\n0.93\\n0.83\\n0.84\\n0.81\\n0.99\\n0.92\\n0.77\\n0.8\\n0.78\\n0.78\\n0.72\\n0.63\\n0.63\\n0.65\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\ndepth-0\\ndepth-1\\ndepth-2\\ndepth-3\\ndepth-5\\nAccuracy\\nStandard Prompting\\nChain-of-Thought Prompting\\nHypothesis Testing Prompting\\n(a)\\n0.6\\n0.41\\n0.39\\n0.34\\n0.33\\n0.77\\n0.54\\n0.58\\n0.56\\n0.5\\n0.82\\n0.63\\n0.62\\n0.61\\n0.57\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\ndepth-0\\ndepth-1\\ndepth-2\\ndepth-3\\ndepth-5\\nAccuracy\\nStandard Prompting\\nChain-of-Thought Prompting\\nHypothesis Testing Prompting\\n(b)\\nFigure 3: Prediction accuracy results on the (a) RuleTaker and (b) ProofWriter datasets.\\n0.74\\n0.26\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nHypothesis Testing\\nChain-of-Thought\\n(a) The proof accuracy of Chain-of-Thought and Hy-\\npothesis Testing prompting.\\n0.65\\n0.3\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nHypothesis Testing\\nChain-of-Thought\\n(b) Comparison of accuracy between Chain-of-\\nThought and Hypothesis Testing prompting on \"Un-\\nknown\" label.\\nFigure 4: Further results on ProofWriter.\\ndatasets that were analyzed show a weakness in\\nall methods for handling \"Unknown\" labels. This\\nbeacuse the OWA hypothesis necessitates the ex-\\nclusion of both positive and negative findings to\\nvalidate the \"Unknown\" label. The advantages of\\nour strategy are illustrated by the comparison of the\\nmodel output outputs in Figure 2. The content \"First\\nassume the conclusion is True ... Then assume\\nthe conclusion is False ... So ... is Unknown.\" gen-\\nerated by the model through learning Hypothesis\\nTesting prompting is more in line with our thinking.\\nBesides, we’ll conduct further research and show\\nit later.\\n4.3.\\nFurther Analysis\\nWe carry out the following thorough analysis to\\nbetter comprehend the thought process:\\nProof Accuracy. Five students are required to\\nmanually evaluate the outcomes of the intermediate\\nreasoning after we randomly picked 100 examples\\nfrom depth-5 of the ProofWriter. Proof accuracy rep-\\nresents the proportion where the inference process\\nhas been proven to be reasonable in the correct\\npart of data label prediction. We compare the re-\\nsults of Chian-of-Thought and Hypothesis Testing\\nprompting and report in Figure 4(a). While Hy-\\npothesis Testing prompting mostly produced the\\ncorrect intermediate reasoning process when the\\npredicted label was correct, CoT only generated\\nthe correct chain for 26% of the examples. This\\nresult is in line with other research showing that\\nLMs rely on spurious correlations when solving log-\\nical problems from beginning to end. Additionally,\\nour approach can successfully increase reason-\\ning’s rationality. In processing the \"Unknown\" label,\\nHypothesis Testing prompting performs noticeably\\nbetter than Chain-of-Thought.\\n\"Unknown\" accuracy.\\nIn the ProofWriter\\ndataset, we separately counted the accuracy of\\nthe \"Unknown\" label shown in Figure 4(b). The\\nresults point to a flaw in the Chain-of-Thought strat-\\negy’s handling of \"Unknown\" labels(only 0.3 accu-\\nracyies). Contrarily, Hypothesis Testing prompting\\nsignificantly increases the reliability of judging this\\nlabel (up to 0.65). This further illustrates the value\\nof holding various assumptions, as well as the re-\\nverse confirmation of conclusions.\\n5.\\nConclusion\\nWe have investigated Hypothesis Testing prompt-\\ning as a straightforward and widely applicable tech-\\nnique for improving deductive reasoning in large\\nlanguage models. Multiple assumptions are made\\nduring hypothesis testing, and conclusions are\\nreverse-validated to arrive at the one and only ac-\\ncurate answer. Through experiments on two logical\\nreasoning datasets, we find that Hypothesis Test-\\ning prompting allows large language models to con-\\nstruct reasoning more reasonably and accurately.\\nWe anticipate that additional research on language-\\nbased reasoning approaches will be stimulated by\\nour novel prompting design strategy.\\n6.\\nReferences\\nAlfred V. Aho and Jeffrey D. Ullman. 1972. The\\nTheory of Parsing, Translation and Compiling,\\nvolume 1. Prentice-Hall, Englewood Cliffs, NJ.\\nAmerican Psychological Association. 1983. Publi-\\ncations Manual. American Psychological Associ-\\nation, Washington, DC.\\nRie Kubota Ando and Tong Zhang. 2005. A frame-\\nwork for learning predictive structures from multi-\\nple tasks and unlabeled data. Journal of Machine\\nLearning Research, 6:1817–1853.\\nGalen Andrew and Jianfeng Gao. 2007. Scalable\\ntraining of L1-regularized log-linear models. In\\nProceedings of the 24th International Conference\\non Machine Learning, pages 33–40.\\nRebecca Bevans. 2022.\\nHypothesis Testing |\\nA Step-by-Step Guide with Easy Examples.\\nScribbr.\\nTom B. Brown, Benjamin Mann, Nick Ryder,\\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse,\\nMark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher\\nBerner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language\\nmodels are few-shot learners. In NeurIPS.\\nBSI. 1973a.\\nNatural Fibre Twines, 3rd edition.\\nBritish Standards Institution, London. BS 2570.\\nBSI. 1973b. Natural fibre twines. BS 2570, British\\nStandards Institution, London. 3rd. edn.\\nA. Castor and L. E. Pollux. 1992. The use of user\\nmodelling to guide inference and learning. Ap-\\nplied Intelligence, 2(1):37–53.\\nAshok K. Chandra, Dexter C. Kozen, and Larry J.\\nStockmeyer. 1981. Alternation. Journal of the As-\\nsociation for Computing Machinery, 28(1):114–\\n133.\\nJ.L. Chercheur. 1994. Case-Based Reasoning, 2nd\\nedition. Morgan Kaufman Publishers, San Mateo,\\nCA.\\nYejin Choi. 2022. The curious case of common-\\nsense intelligence. Daedalus.\\nN. Chomsky. 1973. Conditions on transformations.\\nIn A festschrift for Morris Halle, New York. Holt,\\nRinehart & Winston.\\nAakanksha Chowdhery, Sharan Narang, Jacob\\nDevlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung,\\nCharles Sutton, Sebastian Gehrmann, Parker\\nSchuh, Kensen Shi, Sasha Tsvyashchenko,\\nJoshua Maynez, Abhishek Rao, Parker Barnes,\\nYi Tay,\\nNoam Shazeer,\\nVinodkumar Prab-\\nhakaran, Emily Reif, Nan Du, Ben Hutchinson,\\nReiner Pope, James Bradbury, Jacob Austin,\\nMichael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Gar-\\ncia, Vedant Misra, Kevin Robinson, Liam Fe-\\ndus, Denny Zhou, Daphne Ippolito, David Luan,\\nHyeontaek Lim, Barret Zoph, Alexander Spiri-\\ndonov, Ryan Sepassi, David Dohan, Shivani\\nAgrawal, Mark Omernick, Andrew M. Dai, Thanu-\\nmalayan Sankaranarayana Pillai, Marie Pellat,\\nAitor Lewkowycz, Erica Moreira, Rewon Child,\\nOleksandr Polozov, Katherine Lee, Zongwei\\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\\nOrhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav\\nPetrov, and Noah Fiedel. 2022. Palm: Scaling\\nlanguage modeling with pathways. CoRR.\\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\\n2020. Transformers as soft reasoners over lan-\\nguage. In IJCAI.\\nJames W. Cooley and John W. Tukey. 1965. An\\nalgorithm for the machine calculation of complex\\nFourier series.\\nMathematics of Computation,\\n19(90):297–301.\\nAntonia Creswell, Murray Shanahan, and Irina Hig-\\ngins. 2022. Selection-inference: Exploiting large\\nlanguage models for interpretable logical reason-\\ning. CoRR.\\nYilun Du, Shuang Li, Joshua B. Tenenbaum, and\\nIgor Mordatch. 2022. Learning iterative reason-\\ning through energy minimization. In ICML.\\nUmberto Eco. 1990. The Limits of Interpretation.\\nIndian University Press.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022.\\nComplexity-based\\nprompting for multi-step reasoning. CoRR.\\nDan Gusfield. 1997. Algorithms on Strings, Trees\\nand Sequences. Cambridge University Press,\\nCambridge, UK.\\nJulia Hirschberg and Christopher D. Manning. 2015.\\nAdvances in natural language processing. Sci-\\nence.\\nPaul Gerhard Hoel. 1971a. Elementary Statistics,\\n3rd edition. Wiley series in probability and math-\\nematical statistics. Wiley, New York, Chichester.\\nISBN 0 471 40300.\\nPaul Gerhard Hoel. 1971b. Elementary Statistics,\\n3rd edition, Wiley series in probability and mathe-\\nmatical statistics, pages 19–33. Wiley, New York,\\nChichester. ISBN 0 471 40300.\\nShima Imani, Liang Du, and Harsh Shrivastava.\\n2023. Mathprompter: Mathematical reasoning\\nusing large language models. CoRR.\\nOtto Jespersen. 1922. Language: Its Nature, De-\\nvelopment, and Origin. Allen and Unwin.\\nPhil Johnson-Laird. 2010. Deductive reasoning.\\nWiley Interdisciplinary Reviews: Cognitive Sci-\\nence.\\nPhilip N Johnson-Laird. 1999. Deductive reasoning.\\nAnnual review of psychology.\\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze\\nBrahman, Chandra Bhagavatula, Ronan Le Bras,\\nand Yejin Choi. 2022. Maieutic prompting: Logi-\\ncally consistent reasoning with recursive expla-\\nnations. In EMNLP.\\nJared Kaplan, Sam McCandlish, Tom Henighan,\\nTom B. Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\\nAmodei. 2020. Scaling laws for neural language\\nmodels. CoRR.\\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bha-\\ntia, Xin Xu, and Deepak Ramachandran. 2022.\\nLAMBADA: backward chaining for automated\\nreasoning in natural language. CoRR.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid,\\nYutaka Matsuo, and Yusuke Iwasawa. 2022.\\nLarge language models are zero-shot reason-\\ners. In NeurIPS.\\nRosa Patricio S. La, Brooks J. Paul, Deych Elena,\\nEdward L. Boone, David J. Edwards, Wang Qin,\\nSodergren Erica, Weinstock George, William D.\\nShannon, and Ethan P. White. 2012. Hypothe-\\nsis testing and power calculations for taxonomic-\\nbased human microbiome data. Plos One.\\nChia-Hsuan Lee, Hao Cheng, and Mari Osten-\\ndorf. 2021. Dialogue state tracking with a lan-\\nguage model using schema-driven prompting. In\\nEMNLP. Association for Computational Linguis-\\ntics.\\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu,\\nQiji Zhou, and Yue Zhang. 2023a. Evaluating the\\nlogical reasoning ability of chatgpt and GPT-4.\\nCoRR.\\nHanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli\\nZhang, Qiji Zhou, and Yue Zhang. 2023b. Logi-\\ncot: Logical chain-of-thought instruction-tuning\\ndata collection with GPT-4. CoRR.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\\nJiang, Hiroaki Hayashi, and Graham Neubig.\\n2023c. Pre-train, prompt, and predict: A sys-\\ntematic survey of prompting methods in natural\\nlanguage processing. ACM Comput. Surv.\\nMohammad Sadegh Rasooli and Joel R. Tetreault.\\n2015. Yara parser: A fast and accurate depen-\\ndency parser. Computing Research Repository,\\narXiv:1503.06733. Version 2.\\nRaymond Reiter. 1981. On closed world data bases.\\nIn Readings in Artificial Intelligence.\\nStuart J. Russell and Peter Norvig. 2010. Artificial\\nIntelligence - A Modern Approach, Third Interna-\\ntional Edition. Pearson Education.\\nAbulhair Saparov and He He. 2022. Language mod-\\nels are greedy reasoners: A systematic formal\\nanalysis of chain-of-thought. CoRR.\\nCharles Joseph Singer, E. J. Holmyard, and A. R.\\nHall, editors. 1954–58. A history of technology.\\nOxford University Press, London. 5 vol.\\nJannik Strötgen and Michael Gertz. 2012. Temporal\\ntagging on different domains: Challenges, strate-\\ngies, and gold standards. In Proceedings of the\\nEight International Conference on Language Re-\\nsources and Evaluation (LREC’12), pages 3746–\\n3753, Istanbul, Turkey. European Language Re-\\nsource Association (ELRA).\\nS. Superman, B. Batman, C. Catwoman, and S. Spi-\\nderman. 2000. Superheroes experiences with\\nbooks, 20th edition. The Phantom Editors Asso-\\nciates, Gotham City.\\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark.\\n2021.\\nProofwriter:\\nGenerating implications,\\nproofs, and abductive statements over natural\\nlanguage. In Findings of ACL.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\\nNoam Shazeer, Apoorv Kulshreshtha, Heng-\\nTze Cheng, Alicia Jin, Taylor Bos, Leslie\\nBaker, Yu Du, YaGuang Li, Hongrae Lee,\\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo\\nMenegali, Yanping Huang, Maxim Krikun, Dmitry\\nLepikhin, James Qin, Dehao Chen, Yuanzhong\\nXu, Zhifeng Chen, Adam Roberts, Maarten\\nBosma, Yanqi Zhou, Chung-Ching Chang, Igor\\nKrivokon, Will Rusch, Marc Pickett, Kathleen S.\\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\\nDoshi, Renelito Delos Santos, Toju Duke, Johnny\\nSoraker, Ben Zevenbergen, Vinodkumar Prab-\\nhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-\\nson, Alejandra Molina, Erin Hoffman-John, Josh\\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\\nEd H. Chi, and Quoc Le. 2022. Lamda: Lan-\\nguage models for dialog applications. CoRR.\\nXuezhi Wang, Jason Wei, Dale Schuurmans,\\nQuoc V. Le, Ed H. Chi, and Denny Zhou. 2022.\\nSelf-consistency improves chain of thought rea-\\nsoning in language models. CoRR.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\\ngatama, Maarten Bosma, Denny Zhou, Donald\\nMetzler, Ed H. Chi, Tatsunori Hashimoto, Oriol\\nVinyals, Percy Liang, Jeff Dean, and William Fe-\\ndus. 2022a. Emergent abilities of large language\\nmodels. Trans. Mach. Learn. Res.\\nJason Wei, Xuezhi Wang, Dale Schuurmans,\\nMaarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\\nQuoc V. Le, and Denny Zhou. 2022b. Chain-\\nof-thought prompting elicits reasoning in large\\nlanguage models. In NeurIPS.\\nZhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng,\\nSongyang Gao, Tao Gui, Qi Zhang, and Xuanjing\\nHuang. 2023. Self-polish: Enhance reasoning in\\nlarge language models via problem refinement.\\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiao-\\ntian Han, Qizhang Feng, Haoming Jiang, Bing\\nYin, and Xia Hu. 2023. Harnessing the power of\\nllms in practice: A survey on chatgpt and beyond.\\nCoRR.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\\nGoodman. 2022. Star: Bootstrapping reasoning\\nwith reasoning. In NeurIPS.\\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhen-\\nguo Li, and Yu Li. 2023. Progressive-hint prompt-\\ning improves reasoning in large language mod-\\nels.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason\\nWei, Nathan Scales, Xuezhi Wang, Dale Schuur-\\nmans, Olivier Bousquet, Quoc Le, and Ed H. Chi.\\n2022. Least-to-most prompting enables complex\\nreasoning in large language models. CoRR.\\n'),\n",
       " Document(metadata={'Published': '2024-08-07', 'Title': 'Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs', 'Authors': 'Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, Bing Yin, Yizhou Sun', 'Summary': \"Reasoning encompasses two typical types: deductive reasoning and inductive\\nreasoning. Despite extensive research into the reasoning capabilities of Large\\nLanguage Models (LLMs), most studies have failed to rigorously differentiate\\nbetween inductive and deductive reasoning, leading to a blending of the two.\\nThis raises an essential question: In LLM reasoning, which poses a greater\\nchallenge - deductive or inductive reasoning? While the deductive reasoning\\ncapabilities of LLMs, (i.e. their capacity to follow instructions in reasoning\\ntasks), have received considerable attention, their abilities in true inductive\\nreasoning remain largely unexplored. To investigate into the true inductive\\nreasoning capabilities of LLMs, we propose a novel framework, SolverLearner.\\nThis framework enables LLMs to learn the underlying function (i.e., $y =\\nf_w(x)$), that maps input data points $(x)$ to their corresponding output\\nvalues $(y)$, using only in-context examples. By focusing on inductive\\nreasoning and separating it from LLM-based deductive reasoning, we can isolate\\nand investigate inductive reasoning of LLMs in its pure form via SolverLearner.\\nOur observations reveal that LLMs demonstrate remarkable inductive reasoning\\ncapabilities through SolverLearner, achieving near-perfect performance with ACC\\nof 1 in most cases. Surprisingly, despite their strong inductive reasoning\\nabilities, LLMs tend to relatively lack deductive reasoning capabilities,\\nparticularly in tasks involving ``counterfactual'' reasoning.\"}, page_content='Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities\\nof LLMs\\nKewei Cheng1,2, Jingfeng Yang2, Haoming Jiang2, Zhengyang Wang2, Binxuan Huang2\\nRuirui Li2, Shiyang Li2, Zheng Li2, Yifan Gao2, Xian Li2, Bing Yin2, Yizhou Sun1\\n1University of California, Los Angeles 2Amazon\\n{chenkewe, jingfe, jhaoming, zhengywa, binxuan, ruirul,\\nsyangli, amzzhe, yifangao, xianlee, alexbyin}@amazon.com\\nyzsun@cs.ucla.edu\\nAbstract\\nReasoning encompasses two typical types: de-\\nductive reasoning and inductive reasoning. De-\\nspite extensive research into the reasoning ca-\\npabilities of Large Language Models (LLMs),\\nmost studies have failed to rigorously differenti-\\nate between inductive and deductive reasoning,\\nleading to a blending of the two. This raises an\\nessential question: In LLM reasoning, which\\nposes a greater challenge - deductive or induc-\\ntive reasoning? While the deductive reasoning\\ncapabilities of LLMs, (i.e. their capacity to\\nfollow instructions in reasoning tasks), have\\nreceived considerable attention, their abilities\\nin true inductive reasoning remain largely unex-\\nplored. To investigate the true inductive reason-\\ning capabilities of LLMs, we propose a novel\\nframework, SolverLearner. This framework\\nenables LLMs to learn the underlying function\\n(i.e., 𝑦= 𝑓𝑤(𝑥)), that maps input data points\\n(𝑥) to their corresponding output values (𝑦),\\nusing only in-context examples. By focusing\\non inductive reasoning and separating it from\\nLLM-based deductive reasoning, we can isolate\\nand investigate inductive reasoning of LLMs in\\nits pure form via SolverLearner. Our observa-\\ntions reveal that LLMs demonstrate remarkable\\ninductive reasoning capabilities through Solver-\\nLearner, achieving near-perfect performance\\nwith ACC of 1 in most cases. Surprisingly, de-\\nspite their strong inductive reasoning abilities,\\nLLMs tend to relatively lack deductive reason-\\ning capabilities, particularly in tasks involving\\n“counterfactual” reasoning.\\n1\\nIntroduction\\nRecent years have witnessed notable progress in\\nNatural Language Processing (NLP) with the de-\\nvelopment of Large Language Models (LLMs) like\\nGPT-3 (Brown et al., 2020) and ChatGPT (Ope-\\nnAI, 2023). While these models exhibit impressive\\nreasoning abilities across various tasks, they face\\nchallenges in certain domains. For example, a re-\\ncent study (Wu et al., 2023) has shown that while\\nLLMs excel in conventional tasks (e.g., base-10\\narithmetic), they often experience a notable decline\\nin accuracy when dealing “counterfactual” reason-\\ning tasks that deviate from the conventional cases\\nseen during pre-training (e.g., base-9 arithmetic).\\nIt remains unclear whether they are capable of fun-\\ndamental reasoning, or just approximate retrieval.\\nIn light of this, our paper seeks to investigate\\nthe reasoning capabilities of LLMs. Reasoning\\ncan encompasses two types: deductive reasoning\\nand inductive reasoning, as depicted in Fig. 1. De-\\nductive reasoning starts with a general hypothesis\\nand proceeds to derive specific conclusions about\\nindividual instances while inductive reasoning in-\\nvolves formulating broad generalizations or princi-\\nples from a set of instance observations. Despite\\nextensive research into the reasoning capabilities of\\nLLMs, most studies have not clearly differentiated\\nbetween inductive and deductive reasoning. For in-\\nstance, arithmetic reasoning task primarily focuses\\non comprehending and applying mathematical con-\\ncepts to solve arithmetic problems, aligning more\\nwith deductive reasoning. Yet, when employing\\nin-context learning for arithmetic reasoning tasks,\\nwhere the model is prompted with a few 〈input,\\noutput〉examples, the observed improvements are\\noften attributed to their inductive reasoning capac-\\nity. This fusion of reasoning types poses a critical\\nquestion: Which is the more significant limita-\\ntion in LLM reasoning, deductive or inductive\\nreasoning?\\nTo explore this question, it’s crucial to differen-\\ntiate between deductive and inductive reasoning.\\nCurrent methods that investigate deductive and in-\\nductive reasoning often rely on disparate datasets,\\nmaking direct comparisons challenging (Xu et al.,\\n2023a; Tang et al., 2023; Dalvi et al., 2021; Han\\net al., 2022; Sinha et al., 2019; Yu et al., 2020). To\\novercome this limitation, we have designed a set of\\ncomparative experiments that utilize a consistent\\ntask across different contexts, each emphasizing\\narXiv:2408.00114v2  [cs.AI]  7 Aug 2024\\n(b) Few-shot IO w/\\xa0Mapping Function\\nQ: Assuming that all numbers are in base-8 where\\nthe digits are \"01234567\", what is 57+27?\\nA:\\xa0The result for 57+27 is 106.\\nQ:\\xa0Assuming that all numbers are in base-8 where\\nthe digits are \"01234567\", what is 36+33?\\nA:\\n(Output) The result for 36+33 is 71.\\n(a) Zero-shot\\nQ: Assuming that all numbers are in\\nbase-8 where the digits are\\n\"01234567\", what is 36+33?\\xa0\\nA:\\n(Output) 71\\nDeductive Reasoning\\nInductive Reasoning\\nGeneral Principle\\nSpecific Conclusion\\nThe result for\\xa036+33 is 71.\\nGeneral\\xa0➔\\xa0Specific\\nDeductive\\nReasoning\\n(c) Few-shot IO w/o\\xa0Mapping Function\\nQ:\\xa0You are asked to add two numbers, the\\nbase of which is unknown, what is 57+27?\\nA:\\xa0The result for 57+27 is 106.\\nQ:\\xa0You are asked to add two numbers, the\\nbase of which is unknown,\\xa0what is 36+33?\\nA:\\n(Output) The result for 36+33 is 71.\\n(d) SolverLearner\\nQ:\\xa0You are asked to add two numbers, the base of which is unknown, what is 57+27?\\nA:\\xa0The result for 57+27 is 106.\\nQ:\\xa0What is the function to map the input to the output?\\nA:\\n(Output)\\xa0def solver(n1: str, n2: str) -> str:\\\\n # Let\\'s analyze the given examples to find\\nthe base\\\\n # 57 + 27 = 106\\\\n\\xa0# It seems like the base is 8 (octal)\\\\n\\\\n # Convert the\\ninput strings to integers in base 8\\\\n num1 = int(n1, 8)\\\\n num2 = int(n2, 8)\\\\n\\\\n #\\nCalculate the sum\\\\n result = num1 + num2\\\\n\\\\n # Convert the result back to a string in\\nbase 8\\\\n return oct(result)[2:]\\nAddition in base 8\\nBegin from the rightmost digit, perform the addition. If the sum\\nexceeds 8, subtract 8, record the remainder, and carry over 1\\nto the next column. Repeat this process from right to left for\\neach column, and your final result will be the sum in base 8.\\nThe result for 71+44 is 135.\\nThe result for 42+70 is 132.\\nThe result for 50+45 is 115.\\nThe result for 61+55 is 136.\\nThe result for 63+22 is 105.\\nSpecific\\xa0➔\\xa0Specific\\nSpecific\\xa0➔\\xa0General\\nGeneral\\xa0+\\xa0Specific\\xa0➔\\xa0Specific\\nSpecific Observation\\nGeneral Principle\\nAddition in base 8\\nBegin from the rightmost digit, perform the addition. If the sum\\nexceeds 8, subtract 8, record the remainder, and carry over 1\\nto the next column. Repeat this process from right to left for\\neach column, and your final result will be the sum in base 8.\\nInductive\\nReasoning\\nIntegrate the deductive reasoning with few-shot\\nexamples\\nTraditional IO\\xa0prompting for inductive\\nreasoning\\xa0\\nCompletely decouple inductive reasoning from deductive reasoning\\nDeductive Reasoning\\nDeductive Setting (mapping function is provided)\\nInductive Setting (mapping function is not provided)\\nFigure 1: We have designed a set of comparative experiments that utilize a consistent task across different contexts, each\\nemphasizing either deductive (i.e., methods (a) and (b)) or inductive reasoning (i.e., methods (c) and (d)). As we move from left\\nto right across the figure, the methods gradually transition their primary focus from deductive reasoning to inductive reasoning.\\nSpecifically, method (a) is designed to demonstrate the LLMs’ deductive reasoning in its pure form. Conversely, method (c)\\nutilizes Input-Output (IO) prompting strategies, which are prevalent for probing the inductive reasoning skills of LLMs. However,\\nwe can observe that methods (c) cannot fully disentangle inductive reasoning from deductive reasoning as their learning process\\ndirectly moves from observations to specific instances, blurring the lines between the two. To exclusively focus on and examine\\ninductive reasoning, we introduce a novel framework called SolverLearner, positioned at the far right of the spectrum.\\neither deductive (i.e., methods (a) and (b)) or induc-\\ntive reasoning (i.e., methods (c) and (d)), as depicted\\nin Fig 1. For instance, in an arithmetic task, the pro-\\nficiency of a LLM in deductive reasoning depends\\non its ability to apply a given input-output mapping\\nfunction to solve problems when this function is\\nexplicitly provided. Conversely, an LLM’s skill\\nin inductive reasoning is measured by its ability\\nto infer these input-output mapping functions (i.e.,\\n𝑦= 𝑓𝑤(𝑥)), that maps input data points (𝑥) to their\\ncorresponding output values (𝑦), based solely on\\nin-context examples. The base system often serves\\nas the input-output mapping function in an arith-\\nmetic task. In line with the aforementioned setup,\\nwe employ four methods to investigate the reason-\\ning capacity of LLMs. As we move from left to\\nright across Fig. 1, the methods gradually transition\\ntheir primary focus from deductive reasoning to\\ninductive reasoning. Method (a), at the far left of\\nthe figure, aims to explore the deductive reasoning\\ncapabilities of LLMs in its pure form, where no in-\\ncontext-learning examples are provided (zero-shot\\nsettings). While exploring deductive reasoning in\\nits pure form appears relatively straightforward in\\nzero-shot settings, untangling inductive reasoning\\nposes more significant challenges. Recent studies\\nhave investigated the inductive reasoning abilities\\nof LLMs (Yang et al., 2022; Gendron et al., 2023;\\nXu et al., 2023b), they have primarily used Input-\\nOutput (IO) prompting (Mirchandani et al., 2023),\\nwhich involves providing models with a few 〈in-\\nput, output〉as demonstrations without providing\\nthe underlying mapping function.\\nThe models\\nare then evaluated based on their ability to han-\\ndle unseen examples, as illustrated in method (c).\\nThese studies often find LLMs facing difficulties\\nwith inductive reasoning. Our research suggests\\nthat the use of IO prompting might not effectively\\nseparate LLMs’ deductive reasoning skills from\\ntheir inductive reasoning abilities. This is because\\nthe approach moves directly from observations to\\nspecific instances, obscuring the inductive reason-\\ning steps. Consequently, the underperformance in\\nthe context of inductive reasoning tasks may be\\nattributed to poor deductive reasoning capabilities,\\ni.e., the ability of LLMs to execute tasks, rather than\\nbeing solely indicative of their inductive reasoning\\ncapability.\\nTo disentangle inductive reasoning from deduc-\\ntive reasoning, we propose a novel model, referred\\nto as SolverLearner. Given our primary focus on in-\\nductive reasoning, SolverLearner follows a two-step\\nprocess to segregate the learning of input-output\\nmapping functions from the application of these\\nfunctions for inference. Specifically, functions are\\napplied through external interpreters, such as code\\ninterpreters, to avoid incorporating LLM-based\\ndeductive reasoning.\\nWe evaluate the performance of several LLMs\\nacross various tasks. LLMs consistently demon-\\nstrate remarkable inductive reasoning capabilities\\nthrough SolverLearner, achieving near-perfect per-\\nformance with ACC of 1 in most cases. Surprisingly,\\ndespite their strong inductive reasoning abilities,\\nLLMs tend to exhibit weaker deductive capabilities,\\nparticularly in terms of “counterfactual” reasoning.\\nThis finding, though unexpected, aligns with the\\nprevious research (Wu et al., 2023). In a zero-shot\\nscenario, the ability of an LLM to correctly exe-\\ncute tasks by applying principles (i.e. deductive\\nreasoning) heavily relies on the frequency with\\nwhich the model was exposed to the tasks during\\nits pre-training phase.\\n2\\nTask Definition\\nOur research is focused on a relatively unexplored\\nquestion: Which presents a greater challenge to\\nLLMs - deductive reasoning or inductive reasoning?\\nTo explore this, we designed a set of comparative\\nexperiments that apply a uniform task across var-\\nious contexts, each emphasizing either deductive\\nor inductive reasoning. The primary distinction\\nbetween the deductive and inductive settings is\\nwhether we explicitly present input-output map-\\npings to the models. Informally, we can describe\\nthese mappings as a function 𝑓𝑤: 𝑋→𝑌, where\\nan input 𝑥∈𝑋is transformed into an output 𝑦∈𝑌.\\nWe distinguish between the deductive and inductive\\nsettings as follows:\\n• Deductive setting: we provide the models with\\ndirect input-output mappings (i.e., 𝑓𝑤).\\n• Inductive setting: we offer the models a few\\nexamples (i.e., (𝑥, 𝑦) pairs) while intentionally\\nleaving out input-output mappings (i.e., 𝑓𝑤).\\nFor example, consider arithmetic tasks, where the\\nbase system is the input-output mapping function.\\nThe two approaches on the left side of Fig. 1 (i.e.,\\nmethod (a) and (b)) follow the deductive setting,\\nillustrating the case where the arithmetic base is\\nexplicitly provided. In contrast, the two methods\\n(i.e., method (c) and (d)) on right side of Fig. 1\\nadhere to the inductive setting, depicting the sce-\\nnario characterized by the absence of a specified\\narithmetic base, while a few input-output examples\\nare provided for guidance.\\n3\\nOur Framework for Inductive\\nReasoning: SolverLearner\\nWhile recent studies have explored the inductive\\nreasoning abilities of LLMs (Yang et al., 2022; Gen-\\ndron et al., 2023; Xu et al., 2023b), they have primar-\\nily relied on Input-Output (IO) prompting (Mirchan-\\ndani et al., 2023). This method involves providing\\nmodels with a few 〈input, output〉demonstrations\\nand then evaluating their performance on unseen\\nexamples, as depicted in method (c) in Fig. 1. Our\\nresearch suggests that the use of IO prompting and\\ndirectly evaluating the final instance performance\\nmight not effectively separate LLMs’ deductive\\nreasoning skills from their inductive reasoning abil-\\nities. This is because the approach moves directly\\nfrom observations to specific instances, obscuring\\nthe inductive reasoning steps. To better disentangle\\ninductive reasoning, we propose a novel framework,\\nSolverLearner. This framework enables LLMs to\\nlearn the function (i.e., 𝑦= 𝑓𝑤(𝑥)), that maps in-\\nput data points (𝑥) to their corresponding output\\nvalues (𝑦), using only in-context examples. By\\nfocusing on inductive reasoning and setting aside\\nLLM-based deductive reasoning, we can isolate and\\ninvestigate inductive reasoning of LLMs in its pure\\nform via SolverLearner. SolverLearner includes\\ntwo-stages as illustrated in Fig. 2:\\n• Function Proposal: In this initial phase, we\\npropose a function, that could be used to map\\ninput data points (𝑥) to their corresponding output\\nvalues (𝑦). This is corresponding to the inductive\\nreasoning process.\\n• Function Execution: In the second phase, the\\nproposed function is applied through external\\ncode interpreters to solve the test queries for\\nevaluation purposes. This phase ensures that\\nthe LLM is fully prevented from engaging in\\ndeductive reasoning.\\n3.1\\nFramework\\nIn this subsection, we will take the arithmetic task\\nas a case study to demonstrate the entire process.\\nFunction Proposal: Given the in-context ex-\\namples, the primary goal of LLMs is to learn a\\nfunction that can map input data points (𝑥) to their\\ncorresponding output values (𝑦). This process of\\nlearning the mapping between inputs and outputs\\nis akin to inductive reasoning, while employing\\nthe learned function to address unseen queries\\naligns with deductive reasoning. In order to sepa-\\nrate inductive reasoning from deductive reasoning,\\nthe execution of the learned function should be\\ncompletely detached from LLMs. To achieve this\\nseparation, external tools such as code interpreters\\nserve as efficient way to execute these functions in-\\ndependently. By encapsulating the learned function\\nwithin Python code, we can effectively detach the\\nduty of deductive reasoning from LLMs, assigning\\nThe result for 71+44 is 135.\\nThe result for 42+70 is 132.\\nThe result for 50+45 is 115.\\nThe result for 61+55 is 136.\\nThe result for 63+22 is 105.\\nThe result for 72+62 is 154.\\nThe result for 57+27 is 106.\\nThe result for 52+76 is 150.\\n8 Shot Examples\\nPython\\xa0Function\\n14+57\\n44+45\\n...\\n61+23\\n22+77\\nTest Queries\\n②\\xa0Function\\xa0\\nExecution\\n① Function\\nProposal\\nFigure 2: An overview of our framework SolverLearner for inductive reasoning. SolverLearner follows a two-step process to\\nsegregate the learning of input-output mapping functions from the application of these functions for inference. Specifically,\\nfunctions are applied through external code interpreters, to avoid incorporating LLM-based deductive reasoning.\\nit solely to these external executors. For instance,\\nin function proposal stage for an arithmetic task,\\nwe have:\\n“You are an expert mathematician and program-\\nmer. You are asked to add two numbers, the base\\nof which is unknown. Below are some provided\\nexamples: The result for 76+76 is 174.\\nPlease identify the underlying pattern to determine\\nthe base being used and implement a solver() func-\\ntion to achieve the goal.\\ndef solver(n1: str, n2: str) -> str:\\n# Let’s write a Python program step by step\\n# Each input is a number represented as a string.\\n# The function computes the sum of these numbers\\nand returns it as a string. ”\\nFunction Execution: In the second phase, func-\\ntions are executed through external code interpreters\\nto solve the test cases for evaluation purposes. These\\ncode interpreters act as “oracle” deductive reason-\\ners, fully preventing the LLM from involving deduc-\\ntive reasoning. This ensures that the final results\\nreflect only the inductive reasoning capability of the\\nLLM. To further decouple the LLM’s influence in\\nthis phase, test cases are generated using a template\\nwithout involving the LLM. More details can be\\nfound in Appendix A.1.3.\\n4\\nTasks\\nIn this section, we provide a brief overview of the\\ntasks under consideration. Our focus is on inves-\\ntigating the reasoning abilities of LLMs in both\\ndeductive and inductive reasoning scenarios. To\\nensure a robust evaluation, we carefully select tasks\\nthat lend themselves well to comparison. Firstly, to\\nprevent LLMs from reciting tasks seen frequently\\nduring pre-training, which could artificially inflate\\nperformance in deductive reasoning, a significant\\nportion of the tasks falls into the category of “coun-\\nterfactual reasoning” tasks. Secondly, in the context\\nof inductive reasoning, where only a few in-context\\nexamples are available without the mapping func-\\ntion, our objective is to learn the function that\\nmaps inputs to outputs based on this restricted\\ndataset. To achieve this, we choose tasks that are\\nwell-constrained, ensuring the existence of a single,\\nunique function capable of fitting this limited data.\\nDetailed descriptions of each task and the prompts\\nused can be found in Appendix A.1 and A.2.\\nArithmetic In this study, we focus on the two-\\ndigit addition task previously explored in the work\\nof\\nWu et al. (2023).\\nWe investigate multiple\\nnumerical bases, specifically base-8, 9, 10, 11, and\\n16 where base 10 corresponds to the commonly\\nobserved case during pretraining. In the context of\\ndeductive reasoning, the base is explicitly provided\\nwithout any accompanying in-context examples,\\nand the LLM is expected to perform the addition\\ncomputation by relying on its inherent deductive\\nreasoning abilities. Conversely, in the context of\\ninductive reasoning, instead of explicitly providing\\nthe base information to LLMs, we provide LLMs\\nsolely with few-shot examples and require them\\nto induce the base through these examples and\\nsubsequently generate a function to solve arithmetic\\nproblems.\\nBasic Syntactic Reasoning In this setting, we\\nconcentrate on tasks related to syntactic recognition\\npreviously explored by Wu et al. (2023). Our\\nobjective is to evaluate LLMs using artificially\\nconstructed English sentences that vary from the\\nconventional subject-verb-object (SVO) word order.\\nFor deductive reasoning, we directly provide the\\nnew word order to LLMs without any contextual\\nexamples, challenging them to identify the subject,\\nverb, and object within this artificial language. In\\ncontrast, for inductive reasoning, we do not give\\nexplicit instructions on the changes in word order.\\nInstead, we introduce sentence pairs where one\\nsentence follows the standard word order, and the\\nother follows a modified sequence. Through this\\nsetting, LLMs are expected to learn the specific\\nchanges made to the word order and then apply this\\nlearned rule to identify the subject, verb, and object\\nwithin new sentences.\\nSpatial Reasoning In this task, we investigate\\nthe spatial reasoning previously investigated by Wu\\net al. (2023). Our specific focus is on modifying\\nthe direction-unit vector mapping and determining\\nthe object coordinates in this revised system. We\\nexplore multiple systems, starting with the com-\\nmonly observed case during pretraining, where up\\ncorresponds to north, down to south, left to west,\\nand right to east. This is compared to coordinate\\nsystems with swapped, rotated, and randomly per-\\nmuted axes. For deductive reasoning, we directly\\nprovide the direction-unit vector mapping without\\nany contextual examples, requiring LLMs to com-\\npute the object coordinates within these systems.\\nConversely, in the context of inductive reasoning, in-\\nstead of directly explaining the changes made to the\\ndirection-unit vector mapping to LLMs, we present\\nLLMs with a few example shots and challenge them\\nto infer the changes made to the mapping. They\\nare then expected to apply this learned function to\\ndetermine the object coordinates in the system.\\nCipher Decryption Under this scenario, we ex-\\nplore an innovative task that we have created, con-\\ncentrating on the decryption of strings encrypted\\nusing specific cipher systems. We have incorpo-\\nrated three particular cipher systems for this ex-\\nploration: the Alphabetically Sorting Cipher the\\nCaesar Cipher and the Morse Cipher. For deduc-\\ntive reasoning, we directly inform LLMs about the\\ncipher system being used, yet we do not offer any\\ncontextual examples. The objective for LLMs is to\\ndecode strings according to these cipher systems.\\nConversely, in the inductive reasoning scenario, our\\ntask involves providing LLMs with several exam-\\nples, each consisting of an encrypted string and\\nits decrypted version. The main challenge for the\\nmodels in this scenario is first to identify what ci-\\npher system was used and then to apply that cipher\\nsystem to decrypt an unseen string.\\n5\\nResults\\nFor each task, we evaluate our proposed Solver-\\nLearner for pure LLM inductive reasoning and\\nother settings using two different models, gpt-3.5-\\nturbo-1106 and gpt-4-1106-preview, which are de-\\nnoted as GPT-3.5 and GPT-4 respectively. Since\\nboth methods are closed-source, we do not provide\\nspecific information about their size, architecture,\\nand pre-training particulars. Our experiments pri-\\nmarily focus on investigating the reasoning abilities\\nof LLMs in both deductive and inductive reasoning\\nscenarios. Therefore, we structure our evaluation\\nacross two distinct settings to highlight each type\\nof reasoning. The formal definition of each setting\\nis provided in Sec. 2. For the deductive setting, two\\nmethods are proposed for investigation:\\n• Zero-shot evaluates deductive reasoning ability\\nof the LLMs in its pure form. It tests the LLM’s\\nability to conclude information about specific\\nindividuals based solely on instructions, without\\nrelying on examples.\\n• 8-IO w/ Mapping Function (MF) follows the\\ndeductive setting but enhances LLM reasoning\\nfurther by incorporating in-context examples. It\\naligns with the most commonly used prompt\\nmethods for enabling LLM reasoning. With the\\ninclusion of in-context examples, this approach\\ncan be seen as leveraging inductive reasoning to\\naugment deductive reasoning.\\nFor the inductive setting, we propose two methods\\nfor evaluation:\\n• 8-IO w/o Mapping Function (MF) aligns with\\ntraditional input-output (IO) prompting methods\\nwidely used to investigate the inductive reasoning\\ncapability of LLMs. However, as this method\\nproceeds directly from a set of observations to\\nspecific target instances, it remains intertwined\\nwith LLM-based deductive reasoning.\\n• 8-shot SolverLearner corresponds to our pro-\\nposed framework for inductive reasoning, capable\\nof evaluating inductive reasoning ability of the\\nLLMs in its pure form. It segregates the learning\\nof input-output mapping functions from the ap-\\nplication of these functions for inference, thereby\\npreventing the blend of LLM-based deductive\\nreasoning into the process.\\nBesides using 8-shot examples, our study also in-\\ncludes experiments with 16-shot examples to assess\\nhow changes in the number of in-context examples\\nimpact the results. Experimental results are given\\nin the Appendix A.3. Generally, the results indicate\\nArithmetic\\nBasic Syntax\\nSpatial\\nCipher Decryption\\n8\\n9\\n10\\n11\\n16\\nBase\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\n0-shot\\n8-IO w/ MF\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nOrder\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\n0-shot\\n8-IO w/ MF\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270 Random\\nCoordinate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\n0-shot\\n8-IO w/ MF\\nAlphabetically Sorting\\nCaesar\\nMorse\\nEncryption System\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nACC\\nGPT-3.5\\n0-shot\\n8-IO w/ MF\\n8\\n9\\n10\\n11\\n16\\nBase\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\n0-shot\\n8-IO w/ MF\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nOrder\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\n0-shot\\n8-IO w/ MF\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270 Random\\nCoordinate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\n0-shot\\n8-IO w/ MF\\nAlphabetically Sorting\\nCaesar\\nMorse\\nEncryption System\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\n0-shot\\n8-IO w/ MF\\nFigure 3: Comparison of the deductive reasoning abilities of LLMs across various tasks. Different methods are illustrated\\nthrough color-coded bars: blue bars indicate the results achieved using Zero-shot, while orange bars show the performance of\\n8-IO w/ Mapping Function (MF).\\n8\\n9\\n10\\n11\\n16\\nBase\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs\\n8-IO w/o MF\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nOrder\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs\\n8-IO w/o MF\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270 Random\\nCoordinate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs\\n8-IO w/o MF\\nAlphabetically Sorting\\nCaesar\\nMorse\\nEncryption System\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs\\n8-IO w/o MF\\n8\\n9\\n10\\n11\\n16\\nBase\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n8-IO w/o MF\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nOrder\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n8-IO w/o MF\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270 Random\\nCoordinate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n8-IO w/o MF\\nAlphabetically Sorting\\nCaesar\\nMorse\\nEncryption System\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n8-IO w/o MF\\nFigure 4: Comparison of the inductive reasoning abilities of LLMs across various tasks. Different methods are illustrated\\nthrough color-coded bars: blue bars indicate the results achieved using our proposed SolverLearner, while orange bars show the\\nperformance of 8-IO w/o Mapping Function (MF).\\n8\\n9\\n10\\n11\\n16\\nBase\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs (inductive) \\n0-shot (deductive)\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nOrder\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs\\n0-shot\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270 Random\\nCoordinate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs\\n0-shot\\nAlphabetically Sorting\\nCaesar\\nMorse\\nEncryption System\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-3.5\\nOurs (inductive) \\n0-shot (deductive)\\n8\\n9\\n10\\n11\\n16\\nBase\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n0-shot\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nOrder\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n0-shot\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270 Random\\nCoordinate\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n0-shot\\nAlphabetically Sorting\\nCaesar\\nMorse\\nEncryption System\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nACC\\nGPT-4\\nOurs\\n0-shot\\nFigure 5: Comparison of the inductive reasoning abilities versus deductive reasoning abilities of LLMs across various\\ntasks. Different methods are illustrated through color-coded bars: blue bars indicate the results achieved using our proposed\\nSolverLearner for inductive reasoning, while orange bars show the performance of Zero-shot for deductive reasoning.\\nthat an increase in the number of in-context exam-\\nples yields only slight improvements across both\\ndeductive and inductive reasoning scenarios. Fur-\\nthermore, we conduct an ablation study concerning\\nour proposed SolverLearner in Appendix A.5 for\\ndeeper insights into its functionality.\\n5.1\\nMain Results\\nThe results for all tasks are presented from Fig. 3\\nthrough Fig. 5. Specifically, Fig. 3 concentrates on\\ncomparing performances in the deductive setting,\\nwhile Fig. 4 examines comparisons in the inductive\\nsetting. Additionally, Fig. 5 focuses on contrasting\\nthe models’ capabilities across deductive and induc-\\ntive setting. For further reference, the prompts used\\nfor all tasks are included in Appendix A.2, and the\\nfull numerical results can be found in Appendix A.3.\\nLLMs exhibit poor deductive reasoning capa-\\nbilities, particularly in “counterfactual” tasks.\\nWe include two methods in Fig. 3, Zero-shot and\\n8-IO w/ Mapping Function (MF), to illustrate the\\ndeductive reasoning capability of LLMs. Our obser-\\nvations reveal that LLMs exhibit relatively weaker\\ndeductive capabilities, especially in “counterfac-\\ntual” tasks, while showing prowers in standard\\ntasks like base-10 arithmetic.\\nThis aligns with\\nfindings reported in (Wu et al., 2023). Integration\\nof in-context examples notably enhances LLMs’\\nperformance in various scenarios, suggesting that\\ntheir improvement stems from the acquisition of\\nknowledge through inductive reasoning from these\\nexamples. This further confirms the exceptional\\ninductive reasoning abilities of LLMs. This com-\\nbined evidence suggests that LLMs face challenges\\nin precisely following instructions and executing\\ncommands, especially when those instructions are\\nrelate to scenarios rarely encountered during their\\npre-training phase.\\nLLMs demonstrate remarkable inductive rea-\\nsoning capabilities through SolverLearner. We\\ninclude two methods in Fig. 4, SolverLearner (Ours)\\nand 8-IO w/o Mapping Function (MF), to illustrate\\nthe inductive reasoning capability of LLMs. While\\n8-IO w/o Mapping Function (MF) struggles with\\ninductive reasoning, SolverLearner consistently\\nachieves perfect performance with an accuracy of\\n1 across all the cases with GPT-4 and succeeds in\\nmost cases when used with GPT-3.5. This discrep-\\nancy arises because the utilization of IO prompting\\nto directly reach conclusions on target instances may\\nnot effectively distinguish between LLMs’ deduc-\\ntive and inductive reasoning skills. By completely\\ndisentangling the inductive reasoning of LLMs,\\nour proposed SolverLearner shows the remarkable\\ninductive reasoning capabilities inherent in LLMs.\\nIt is also noteworthy that the efficacy of LLMs’\\ninductive reasoning capability heavily depends on\\nthe foundational model, with GPT-4 consistently\\noutperforming GPT-3.5.\\nDeductive reasoning presents a greater chal-\\nlenge than inductive reasoning for LLMs. To\\ncompare the challenges of the deductive reasoning\\ncapability with the inductive reasoning capability\\nof LLMs, we include two methods in Fig. 1, Solver-\\nLearner and Zero-shot, demonstrating pure induc-\\ntive and deductive reasoning abilities. Since the\\nentire reasoning involves two steps: first, obtaining\\nthe input-output function ( 𝑓𝑤), which corresponds\\nto inductive reasoning, and second, applying the\\nfunction for inference, which corresponds to deduc-\\ntive reasoning. Once both steps are successfully\\ncompleted, perfect performance is observed, as\\nindicated by the dotted line in the figure. Zero-\\nshot can be seen as replacing the first step with\\nan oracle, with deductive reasoning capability of\\nLLMs to be studied, while SolverLearner can be\\nseen as replacing the second step with an oracle,\\nwith inductive reasoning capability of LLMs to be\\nstudied. By comparing the gaps of SolverLearner\\nand Zero-shot towards perfect reasoning, we can\\nobserve that in most cases, LLMs can complete the\\ninductive step perfectly, while they rarely achieve\\nperfect performance on the deductive step. This in-\\ndicates that in LLM reasoning, deductive reasoning\\npresents a greater challenge. Note that we avoid to\\nphrasing it as directly comparing inductive and de-\\nductive reasoning capabilities. Instead, we examine\\nwhether the gaps mainly come from inductive or\\ninductive reasoning, considering that LLMs could\\nnot achieve perfect counterfactual reasoning.\\n5.2\\nMore Results over Additional LLMs\\nTo validate the generalizability of our conclusion,\\nwe have included results over additional LLMs,\\nclaude-3-sonnet-20240229-v1:0, which is denoted\\nas Claude3.\\nDue to space limitations, the full\\nnumerical results are provided in Appendix A.4.\\n5.3\\nAblation Study\\nWe conducted several experiments to gain a deeper\\nunderstanding of our framework, detailed in the ab-\\nlation studies in Appendix A.5. These experiments\\ninclude investigating the effects of programs exe-\\ncuted by a Python interpreter v.s. natural language\\nexecuted by an LLM and examining the impact of\\nthe number of in-context learning examples.\\n6\\nRelated Works\\n6.1\\nIn-Context Learning\\nGPT-3 (Brown et al., 2020) has demonstrated its\\neffectiveness in learning from a few demonstration\\nexamples and solve previously unseen tasks with-\\nout requiring updates to its model parameters (Wei\\net al., 2022a). This remarkable capability is com-\\nmonly referred to as the “in-context learning ability”\\nof language models. It implies that the LLMs can\\nleverage its existing knowledge and generalize from\\na few demonstration examples to solve new, related\\ntasks (Dong et al., 2022; Liu et al., 2021; Rubin et al.,\\n2021; Gonen et al., 2022). Some notable works\\ninclude chain-of-thought (CoT) prompting (Wei\\net al., 2022b), which elicits reasoning with inter-\\nmediate steps in few-shot exemplars. Built upon\\nthe CoT framework, several works expand CoT by\\norganizing and processing thoughts using more\\ncomplex structures, such as trees (Yao et al., 2023)\\nand graphs (Besta et al., 2023) or breaking a prob-\\nlem into sub problems and then proceeds to solve\\neach one independently (Zhou et al., 2022). While\\nthese studies have effectively improved the reason-\\ning capability of LLMs, they have failed to clearly\\ndistinguish between inductive and deductive reason-\\ning, let alone investigate which represents a more\\ncritical limitation for LLM reasoning capabilities:\\ndeductive reasoning or inductive reasoning.\\n6.2\\nExploring LLMs’ Reasoning Skills\\nDespite the impressive achievements of LLMs in\\nvarious reasoning tasks, the underlying mechanisms\\nof their reasoning capabilities remain a subject of\\ndebate. The question of whether LLMs genuinely\\nreason in a manner akin to human cognitive pro-\\ncesses or merely simulate aspects of reasoning\\nwithout true comprehension is still open (Huang\\nand Chang, 2022).\\nFor instance, Kojima et al.\\nhave suggested that LLMs exhibit commendable\\nzero-shot reasoning abilities, implying that these\\nmodels can draw logical conclusions in scenarios\\nthey have not been explicitly trained on (Kojima\\net al., 2022). However, some researchers cast doubt\\non the reasoning capability of LLMs. While ap-\\nproaches like the chain-of-thought method may\\nmimic human-like thought processes, it remains\\nuncertain whether LLMs are genuinely engaging in\\nreasoning or simply following patterns learned dur-\\ning training (Wei et al., 2022b; Valmeekam et al.,\\n2022). Additionally, there’s a debate regarding\\nwhether LLMs are symbolic reasoners (Tang et al.,\\n2023) or possess strong abstract reasoning capa-\\nbilities (Gendron et al., 2023). In light of these\\nseemingly contradictory conclusions, our research\\naims to investigate deeper into the reasoning capa-\\nbilities of LLMs. We intend to dissect the nuances\\nof inductive and deductive reasoning within the\\ncontext of LLMs, identifying which form of reason-\\ning presents a more significant challenge to their\\nreasoning abilities.\\n6.3\\nEquipping LLMs with External Tools\\nLarge Language Models (LLMs) have made signifi-\\ncant progress in utilizing tools through frameworks\\nlike CREATOR (Qian et al., 2023) and LATM (Cai\\net al., 2023), which allow LLMs to create tools\\nusing documentation and code. Logic-LM (Pan\\net al., 2023) integrates LLMs with symbolic solvers\\nto improve logical problem-solving, However, these\\napproaches focus exclusively on deductive reason-\\ning, aiming to enable LLMs to derive correct an-\\nswers for specific questions without incorporating\\nthe capacity for inductive reasoning to infer underly-\\ning mapping function shared by few-shot examples.\\nIn contrast, our primary objective is not to propose\\na new framework for using tools to enhance the\\nproblem-solving capabilities of LLMs. Instead, we\\naim to differentiate between deductive and inductive\\nreasoning within LLMs and explore which presents\\na greater challenge to their reasoning abilities.\\n7\\nConclusion\\nThis study aims to explore a less-investigated aspect\\nof LLMs: within LLM reasoning, which presents\\na greater challenge — deductive or inductive rea-\\nsoning?\\nTo investigate the inductive reasoning\\ncapacities of LLMs, we introduce a novel frame-\\nwork called SolverLearner. By concentrating on\\ninductive reasoning while setting aside LLM-based\\ndeductive reasoning, SolverLearner can scrutinize\\nthe pure form of inductive reasoning in LLMs.\\nOur findings unveil remarkable inductive reasoning\\nprowers in LLMs through SolverLearner, achieving\\nnear-perfect performance with an ACC of 1 in most\\ncases. Surprisingly, despite their strong inductive\\nreasoning abilities, LLMs often exhibit weaker de-\\nductive capabilities, particularly in tasks involving\\n“counterfactual” scenarios.\\nLimitations\\nLLMs cannot perform inductive reasoning over\\nall the tasks In our inductive learning setting, LLMs\\nare provided with only a limited number of contex-\\ntual examples. The goal is to infer the function that\\naccurately maps inputs to outputs based solely on\\nthis constrained dataset. In order to solve this prob-\\nlem, it is significant that we can find a unique func-\\ntion satisfied given these examples. For instance, a\\nlinear function can be precisely determined given\\njust two data points, as it has a singular solution.\\nHowever, attempting to deduce a quadratic curve\\nfrom two points poses an insurmountable challenge\\ndue to the existence of infinite functions capable\\nof passing through those specific points. Addition-\\nally, LLMs might struggle to discern the correct\\nmapping function when the search space of the\\nproblem expands excessively. Consider the case\\nof arithmetic tasks; without limiting the search\\nspace to finding a suitable base that aligns with\\nthe observations, the task becomes overwhelmingly\\ncomplex. This is because the search space could en-\\ncompass any conceivable rule that accommodates\\nthe observations.\\nThe effectiveness of LLMs’ inductive reason-\\ning capability is heavily reliant on the founda-\\ntional model While GPT-4 consistently showcase\\nimpressive inductive reasoning abilities through\\nSolverLearner and achieve perfect performance\\nwith ACC of 1 across all the tasks, GPT-3.5 strug-\\ngle to learn the correct input-output mapping func-\\ntion in several cases. This observation suggests\\nthat the inductive reasoning potential of LLMs is\\nsignificantly constrained by the underlying model.\\nChain of Thought (COT) has not been incor-\\nporated into the comparison Chain of Thought\\n(COT) is a significant prompting technique designed\\nfor use with LLMs. Rather than providing a direct\\nanswer, COT elicits reasoning with intermediate\\nsteps in few-shot exemplars. This method was not\\nincorporated into our comparison as it is viewed\\nas a technique to improve the deductive reasoning\\ncapabilities of LLMs. Although COT has proven to\\nbe effective across various tasks, numerous studies\\nhighlight a significant performance gap that COT\\nstill needs to bridge to achieve flawless execution.\\nEthical Considerations\\nThe authors foresee no ethical concerns with the\\nresearch presented in this paper.\\nReferences\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\\nLehmann, Michal Podstawski, Hubert Niewiadomski,\\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\\ning elaborate problems with large language models.\\narXiv preprint arXiv:2308.09687.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\\nand Denny Zhou. 2023. Large language models as\\ntool makers. arXiv preprint arXiv:2305.17126.\\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zheng-\\nnan Xie, Hannah Smith, Leighanna Pipatanangkura,\\nand Peter Clark. 2021.\\nExplaining answers with\\nentailment trees. arXiv preprint arXiv:2104.08661.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\\nWu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang\\nSui. 2022. A survey for in-context learning. arXiv\\npreprint arXiv:2301.00234.\\nGaël Gendron, Qiming Bao, Michael Witbrock, and\\nGillian Dobbie. 2023. Large language models are not\\nabstract reasoners. arXiv preprint arXiv:2305.19555.\\nHila Gonen, Srini Iyer, Terra Blevins, Noah A Smith,\\nand Luke Zettlemoyer. 2022. Demystifying prompts\\nin language models via perplexity estimation. arXiv\\npreprint arXiv:2212.04037.\\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting\\nQi, Martin Riddell, Luke Benson, Lucy Sun, Ekate-\\nrina Zubova, Yujie Qiao, Matthew Burtell, et al. 2022.\\nFolio: Natural language reasoning with first-order\\nlogic. arXiv preprint arXiv:2209.00840.\\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\\nwards reasoning in large language models: A survey.\\narXiv preprint arXiv:2212.10403.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid,\\nYutaka Matsuo, and Yusuke Iwasawa. 2022. Large\\nlanguage models are zero-shot reasoners. Advances\\nin neural information processing systems, 35:22199–\\n22213.\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\\nLawrence Carin, and Weizhu Chen. 2021. What\\nmakes good in-context examples for gpt-3? arXiv\\npreprint arXiv:2101.06804.\\nSuvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,\\nDanny Driess, Montserrat Gonzalez Arenas, Kan-\\nishka Rao, Dorsa Sadigh, and Andy Zeng. 2023.\\nLarge language models as general pattern machines.\\narXiv preprint arXiv:2307.04721.\\nOpenAI. 2023.\\nGpt-4 technical report.\\nArXiv,\\nabs/2303.08774.\\nLiangming Pan, Alon Albalak, Xinyi Wang, and\\nWilliam Yang Wang. 2023. Logic-lm: Empower-\\ning large language models with symbolic solvers\\nfor faithful logical reasoning.\\narXiv preprint\\narXiv:2305.12295.\\nCheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan\\nLiu, and Heng Ji. 2023. Creator: Tool creation for\\ndisentangling abstract and concrete reasoning of large\\nlanguage models. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2023, pages\\n6922–6939.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.\\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau,\\nand William L Hamilton. 2019. Clutrr: A diagnostic\\nbenchmark for inductive reasoning from text. arXiv\\npreprint arXiv:1908.06177.\\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng,\\nSong-Chun Zhu, Yitao Liang, and Muhan Zhang.\\n2023. Large language models are in-context seman-\\ntic reasoners rather than symbolic reasoners. arXiv\\npreprint arXiv:2305.14825.\\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\\nand Subbarao Kambhampati. 2022. Large language\\nmodels still can’t plan (a benchmark for llms on\\nplanning and reasoning about change). arXiv preprint\\narXiv:2206.10498.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\\n2022a. Emergent abilities of large language models.\\narXiv preprint arXiv:2206.07682.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\\net al. 2022b.\\nChain-of-thought prompting elicits\\nreasoning in large language models. Advances in\\nNeural Information Processing Systems, 35:24824–\\n24837.\\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek,\\nBoyuan Chen, Bailin Wang, Najoung Kim, Jacob\\nAndreas, and Yoon Kim. 2023. Reasoning or reciting?\\nexploring the capabilities and limitations of language\\nmodels through counterfactual tasks. arXiv preprint\\narXiv:2307.02477.\\nFangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun\\nLiu, and Erik Cambria. 2023a. Are large language\\nmodels really good logical reasoners? a compre-\\nhensive evaluation from deductive, inductive and\\nabductive views. arXiv preprint arXiv:2306.09841.\\nYudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott\\nSanner, and Elias B Khalil. 2023b. Llms and the\\nabstraction and reasoning corpus: Successes, failures,\\nand the importance of object-based representations.\\narXiv preprint arXiv:2305.18354.\\nZonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik\\nCambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei.\\n2022. Language models as inductive reasoners. arXiv\\npreprint arXiv:2212.10923.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models. arXiv\\npreprint arXiv:2305.10601.\\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi\\nFeng. 2020.\\nReclor:\\nA reading comprehension\\ndataset requiring logical reasoning. arXiv preprint\\narXiv:2002.04326.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nClaire Cui, Olivier Bousquet, Quoc Le, et al. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models.\\narXiv preprint\\narXiv:2205.10625.\\nA\\nAppendix\\nA.1\\nFull Setups\\nSolverLearner is a prompting based reasoning ap-\\nproach, and we only need to perform inference with\\nLLMs.\\nA.1.1\\nSettings for Each Task\\nArithmetic The arithmetic dataset introduced in\\nWu et al.’s paper (Wu et al., 2023) comprises 1,000\\nrandomly selected addition expressions, each in-\\nvolving two-digit numbers. These expressions are\\ndrawn from bases 8, 9, 10, 11, and 16, with sepa-\\nrate sampling for each base. Importantly, all the\\nexpressions have been carefully chosen to yield\\ndistinct results when evaluated in their respective\\nbases, thereby distinguishing them from one another\\nduring the process of rule learning.\\nBasic Syntactic Reasoning In accordance with\\nthe methodology outlined in Wu et al.’s work (Wu\\net al., 2023), we have generated a set of 100 simple\\nthree-word sentences (e.g., “bob likes bananas”)\\nwith five different word order variations (e.g., “ba-\\nnanas bob likes” in OSV format). Subsequently,\\nwe tasked LLMs with learning how to manipulate\\nsentence order. It’s noteworthy that we took great\\ncare in selecting words to ensure that each word in\\na sentence can only fulfill one specific role, such as\\nsubject, object, or verb. For instance, we ensured\\nthat sentences like “bob likes anna” were excluded,\\nas both “bob” and “anna” could potentially serve as\\nboth subjects and objects, violating this constraint.\\nSpatial Reasoning The spatial reasoning dataset\\nintroduced in Wu et al.’s paper (Wu et al., 2023)\\nconsists of 100 rooms that were randomly selected,\\nand each room contains three distinct objects. The\\nspatial directions within these rooms are represented\\nusing unit vectors. For instance, north is represented\\nas (0, 1), south as (0, -1), east as (1, 0), and west\\nas (-1, 0), with a y-axis pointing upward serving\\nas the default orientation. In our study, we have\\nmodified the mapping between directions and unit\\nvectors and tasked LLMs with learning this new\\ndirection-to-unit vector relationship. We explore\\ntwo direction-swapped scenarios (north-south and\\neast-west), three rotated scenarios (by 90°, 180°,\\nand 270°), and a randomly permuted scenario. The\\nprimary metric we report is instance-level accuracy,\\nwhich necessitates that all three objects within a\\nroom must be correctly positioned in order to be\\nconsidered accurate.\\nCipher Decryption We’ve generated a collection\\nof 100 pairs of strings (e.g., “Mrxuqhb -> Journey”\\nfor Caesar Cipher) for each of three cipher systems,\\nincluding the Alphabetically Sorting Cipher the\\nCaesar Cipher and the Morse Cipher. Each pair\\ncomprises an encrypted string (e.g., “Mrxuqhb”)\\nand its corresponding decrypted version (e.g., “Jour-\\nney”). By providing LLMs with several examples,\\neach containing an encrypted string alongside its\\ncorresponding decrypted counterpart, the primary\\ntask is to accurately determine the cipher system\\nemployed in an open-world context.\\nA.1.2\\nFew shot Example Generation\\nThe preparation of examples for few-shot learning\\nfollows a straightforward process. We divide all the\\ndata into a training set and a test set, from which few-\\nshot examples are extracted from the training set.\\nThese few-shot examples are automatically prepared\\nby associating queries with their corresponding\\nground truth answers using a pre-defined template.\\nA.1.3\\nTest Case Generation\\nIn the function execution phase, the test cases are\\ngenerated using a template without involving LLM.\\nIn particular, the test cases are drawn from the test\\ndata files, containing all the queries along with\\ntheir correct answers (e.g., “76+76 = 174”). When\\nthe LLM is used for generating code, we specify\\na function interface, such as def solver(n1: str,\\nn2: str) -> str. Then, using the query examples\\nprovided, like “76+76 = 174”, we create test cases\\nby applying this function interface to the query (e.g.,\\nsolver(76,76)), thereby eliminating any reliance on\\nLLM for this process. This method ensures that our\\ntest case generation is 100% correct.\\nA.2\\nFull Prompts\\nWe provide the prompts that we used to query the\\nLLMs for all tasks in Tables 1 to 4. We do not use\\nthe system message field for any model.\\nA.3\\nFull Results\\nWe show the full numerical results in Tables 5 to 8.\\nIn addition to using 8-shot examples, these results\\nalso include experiments with 16-shot examples\\nto assess how changes in the number of in-context\\nexamples impact the results.\\nA.4\\nMore Results on Additional LLMs\\nTo validate the generalizability of our conclu-\\nsion, we have included additional LLMs, claude-\\n3-sonnet-20240229-v1:0, which is denoted as\\nClaude3. We show the full numerical results in\\nTables 9 to 12.\\nA.5\\nAblation studies\\nLLMs struggle as executors when applying\\nlearned functions. To better demonstrate the de-\\nductive capacity of LLM, we present both GPT-3.5\\nand Python with identical code and task them with\\napplying the code to deduce the same set of queries.\\nAs shown in Table 13, while the Python interpreter\\ncan be considered an oracle, delivering flawless\\nperformance, it proves challenging for LLMs to\\naccurately execute the code.\\nLLMs can learn the function with very few\\nexamples when the inductive reasoning problem\\nis well defined.\\nTo examine the impact of the\\nnumber of few-shot examples on the inductive rea-\\nsoning capability of LLMs, we vary the number of\\nin-context examples within [1,2,4,8,16] and assess\\nperformance on the spatial reasoning task using\\nGPT-3.5 as presented in Table 14. We observe that\\neven with very few examples, GPT-3.5 can still\\nlearn the mapping function if it is learnable.\\nTable 1: Prompts for the Arithmetic Task.\\nMode\\nPrompt\\nZero-shot\\nYou are a mathematician. Assuming that all numbers are in base-8 where the digits are \"01234567\",\\nwhat is 36+33? End the response with the result in \"\\\\boxed{result}\".\\nFew-shot IO w/ MF\\nYou are a mathematician. You are asked to add two numbers. Assuming that all numbers are in\\nbase-8 where the digits are \"01234567\". Below are some provided examples:\\nThe result for 76+76 is 174.\\nPlease identify the base being used and determine what is 36+33? End the response with the result\\nin \"\\\\boxed{result}\".\\nFew-shot IO w/o MF\\nYou are a mathematician. You are asked to add two numbers, the base of which is unknown. Below\\nare some provided examples:\\nThe result for 76+76 is 174.\\nPlease identify the base being used and determine what is 36+33? End the response with the result\\nin \"\\\\boxed{result}\".\\nSolverLearner\\nYou are an expert mathematician and programmer. You are asked to add two numbers, the base of\\nwhich is unknown. Below are some provided examples:\\nThe result for 76+76 is 174.\\nPlease identify the underlying pattern to determine the base being used and implement a solver()\\nfunction to achieve the goal.\\ndef solver(n1: str, n2: str) -> str:\\n# Let’s write a Python program step by step\\n# Each input is a number represented as a string.\\n# The function computes the sum of these numbers and returns it as a string.\\nAfter defining the solver() function, create test cases based on the input examples and print the results.\\nAn example of a test case could be \"print(solver(\"76\", \"76\"))\". Place the function solver() as well as\\nthe test cases between \"START_CODE\" and \"END_CODE\".\\nTable 2: Prompts for the Basic Syntactic Reasoning Task.\\nMode\\nPrompt\\nZero-shot\\nYou are an expert in linguistics. Imagine a language that is the same as English with the only\\nexception being that it uses the object-subject-verb order instead of the subject-verb-object order.\\nPlease identity the subject, verb, and object in the following sentences from this invented language:\\nshirts sue hates.\\nEncode the identified subject, verb, and object in the form of a dictionary with the following structure:\\n{’subject’: ?, ’verb’: ?, ’object’: ?}.\\nFew-shot IO w/ MF\\nAs a linguistics expert, your objective is to analyze sentences in a constructed language that shares\\nEnglish vocabulary but uses the object-subject-verb order instead of the subject-verb-object order.\\nPresented below are examples of valid sentences in this constructed language, accompanied by their\\ncorresponding English translations.\\nA sentence in this invented language: phones mary finds. Its equivalent sentence in English reads:\\nmary finds phones.\\nFollowing the examples, please analyze the subject, verb, and object in the following sentences from\\nthis invented language:\\nshirts sue hates.\\nEncode the identified subject, verb, and object in the form of a dictionary with the following structure:\\n{’subject’: ?, ’verb’: ?, ’object’: ?}.\\nFew-shot IO w/o MF\\nAs a linguistics expert, your objective is to analyze sentences in a constructed language that shares\\nEnglish vocabulary but follows a unique grammatical structure. Presented below are examples of valid\\nsentences in this constructed language, accompanied by their corresponding English translations.\\nA sentence in this invented language: phones mary finds. Its equivalent sentence in English reads:\\nmary finds phones.\\nFollowing the examples, please analyze the subject, verb, and object in the following sentences from\\nthis invented language:\\nshirts sue hates.\\nEncode the identified subject, verb, and object in the form of a dictionary with the following structure:\\n{’subject’: ?, ’verb’: ?, ’object’: ?}.\\nSolverLearner\\nAs a linguistics expert, your objective is to analyze sentences in a constructed language that shares\\nEnglish vocabulary but follows a unique grammatical structure.Presented below are examples of valid\\nsentences in this constructed language, accompanied by their corresponding English translations.\\nA sentence in this invented language: phones mary finds. Its equivalent sentence in English reads:\\nmary finds phones.\\nPlease summarize the pattern concerning the order of subject, verb and object in this invented\\nlinguistic system. Place the pattern between START_PATTERN and END_PATTERN.\\nTable 3: Prompts for the Spatial Reasoning Task.\\nMode\\nPrompt\\nZero-shot\\nYou are in the middle of a room. You can assume that the room’s width and height are both 500\\nunits. The layout of the room in the following format:\\n’name’: ’bedroom’, ’width’: 500, ’height’: 500, ’directions’: ’north’: [0, 1], ’south’: [0, -1], ’east’:\\n[1, 0], ’west’: [-1, 0], ’objects’: [’name’: ’chair’, ’direction’: ’east’, ’name’: ’wardrobe’, ’direction’:\\n’north’, ’name’: ’desk’, ’direction’: ’south’]\\nPlease provide the coordinates of objects whose positions are described using cardinal directions,\\nunder a conventional 2D coordinate system using the following format:\\n[’name’: ’chair’, ’x’: ’?’, ’y’: ’?’, ’name’: ’wardrobe’, ’x’: ’?’, ’y’: ’?’, ’name’: ’desk’, ’x’: ’?’, ’y’:\\n’?’]\\nFew-shot IO w/ MF\\nYou are an expert programmer. You are in the middle of a room. You can assume that the room’s\\nwidth and height are both 500 units. The layout of the room in the following format:\\n’name’: ’laundry room’, ’width’: 500, ’height’: 500, ’directions’: ’north’: [0, 1], ’south’: [0, -1],\\n’east’: [1, 0], ’west’: [-1, 0], ’objects’: [’name’: ’dryer’, ’direction’: ’east’, ’name’: ’sink’, ’direction’:\\n’west’, ’name’: ’washing machine’, ’direction’: ’south’]\\nPlease provide the coordinates of objects whose positions are described using cardinal directions,\\nunder a conventional 2D coordinate system. For example, the coordinates of objects in the above\\nexample is:\\n[’name’: ’dryer’, ’x’: 500, ’y’: 250, ’name’: ’sink’, ’x’: 0, ’y’: 250, ’name’: ’washing machine’, ’x’:\\n250, ’y’: 0]\\nFollowing the examples, please give the coordinates of objects in the following room using the same\\nformat:\\n’name’: ’bedroom’, ’width’: 500, ’height’: 500, ’directions’: ’north’: [0, 1], ’south’: [0, -1], ’east’:\\n[1, 0], ’west’: [-1, 0], ’objects’: [’name’: ’chair’, ’direction’: ’east’, ’name’: ’wardrobe’, ’direction’:\\n’north’, ’name’: ’desk’, ’direction’: ’south’]\\nFew-shot IO w/o MF\\nYou are in the middle of a room. You can assume that the room’s width and height are both 500\\nunits. The layout of the room in the following format:\\n’name’: ’laundry room’, ’width’: 500, ’height’: 500, ’objects’: [’name’: ’dryer’, ’direction’: ’east’,\\n’name’: ’sink’, ’direction’: ’west’, ’name’: ’washing machine’, ’direction’: ’south’]\\nPlease provide the coordinates of objects whose positions are described using cardinal directions,\\nunder a conventional 2D coordinate system. For example, the coordinates of objects in the above\\nexample is:\\n[’name’: ’dryer’, ’x’: 500, ’y’: 250, ’name’: ’sink’, ’x’: 0, ’y’: 250, ’name’: ’washing machine’, ’x’:\\n250, ’y’: 0]\\nFollowing the examples, please give the coordinates of objects in the following room using the same\\nformat:\\n’name’: ’bedroom’, ’width’: 500, ’height’: 500, ’objects’: [’name’: ’chair’, ’direction’: ’east’, ’name’:\\n’wardrobe’, ’direction’: ’north’, ’name’: ’desk’, ’direction’: ’south’]\\nSolverLearner\\nYou are an expert programmer. You are in the middle of a room. You can assume that the room’s\\nwidth and height are both 500 units. The layout of the room in the following format: ’name’: ’laundry\\nroom’, ’width’: 500, ’height’: 500, ’objects’: [’name’: ’dryer’, ’direction’: ’east’, ’name’: ’sink’,\\n’direction’: ’west’, ’name’: ’washing machine’, ’direction’: ’south’]\\nPlease provide the coordinates of objects whose positions are described using cardinal directions,\\nunder a conventional 2D coordinate system. For example, the coordinates of objects in the above\\nexample is:\\n[’name’: ’dryer’, ’x’: 500, ’y’: 250, ’name’: ’sink’, ’x’: 0, ’y’: 250, ’name’: ’washing machine’, ’x’:\\n250, ’y’: 0]\\nPlease summarize the pattern and implement a solver() function to achieve the goal.\\ndef solver():\\n# Let’s write a Python program step by step\\n# the input is the layout of the room\\n# the output the coordinates of objects\\nAfter defining the solver() function. Place the function solver() between \"START_CODE\" and\\n\"END_CODE\".\\nTable 4: Prompts for the Cipher Decryption Task.\\nMode\\nPrompt\\nZero-shot\\nAs an expert cryptographer and programmer, your task involves reordering the character sequence\\naccording to the alphabetical order to decrypt secret messages. Please decode the following sequence:\\nspring\\nPlease answer the question by placing the decoded sequence between \"START_DECODING\" and\\n\"END_DECODING\".\\nFew-shot IO w/ MF\\nAs an expert cryptographer and programmer, your task involves reordering the character sequence\\naccording to the alphabetical order to decrypt secret messages. For example, given the sequence\\n\"family,\" you must translate it into \"afilmy.\" Below are further examples that demonstrate the\\ntranslation:\\nschool -> chloos\\nFollowing the examples, please decode the following sequence:\\nspring\\nPlease answer the question by placing the decoded sequence between \"START_DECODING\" and\\n\"END_DECODING\".\\nFew-shot IO w/o MF\\nAs an expert cryptographer and programmer, your task involves deciphering secret messages. For\\nexample, given the sequence \"family,\" you must translate it into \"afilmy.\" Below are further examples\\nthat demonstrate the translation:\\nschool -> chloos\\nFollowing the examples, please decode the following sequence:\\nspring\\nPlease answer the question by placing the decoded sequence between \"START_DECODING\" and\\n\"END_DECODING\".\\nSolverLearner\\nAs an expert cryptographer and programmer, your task involves deciphering secret messages. For\\nexample, given the sequence \"family,\" you must translate it into \"afilmy.\" Below are further examples\\nthat demonstrate the translation:\\nschool -> chloos\\nPlease deduce the encryption system and develop a solver() function for the decryption.\\ndef solver():\\n# Let’s write a Python program step by step\\n# the input is the coded sequence\\n# the output is the decoded sequence\\nAfter defining the solver() function. Place the function solver() between \"START_CODE\" and\\n\"END_CODE\".\\nTable 5: Full Main Results for Arithmetic Task.\\nMethod\\nBase\\n8\\n9\\n10\\n11\\n16\\nGPT-3.5\\nZero-shot\\n0.330\\n0.117\\n1\\n0.066\\n0.294\\n8-IO w/ MF\\n0.376\\n0.089\\n1\\n0.089\\n0.849\\n8-IO w/o MF\\n0.120\\n0.027\\n0.905\\n0.057\\n0.587\\n16-IO w/ MF\\n0.428\\n0.088\\n1\\n0.098\\n0.912\\n16-IO w/o MF\\n0.108\\n0.025\\n0.924\\n0.063\\n0.575\\n8-shot SolverLearner\\n0.571\\n0.462\\n1\\n0.095\\n1\\nGPT-4\\nZero-shot\\n0.600\\n0.697\\n0.999\\n0.551\\n0.819\\n8-IO w/ MF\\n0.576\\n0.717\\n0.860\\n0.540\\n0.862\\n8-IO w/o MF\\n0.255\\n0.268\\n0.545\\n0.264\\n0.431\\n16-IO w/ MF\\n0.543\\n0.720\\n0.817\\n0.534\\n0.840\\n16-IO w/o MF\\n0.257\\n0.245\\n0.505\\n0.237\\n0.435\\n8-shot SolverLearner\\n1\\n1\\n1\\n1\\n1\\nTable 6: Full Main Results for Basic Syntactic Reasoning.\\nMethod\\nWord Order\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nGPT-3.5\\nZero-shot\\n0.560\\n0.298\\n0.190\\n0.226\\n0.560\\n8-IO w/ MF\\n1\\n0.643\\n0.583\\n0.976\\n0.988\\n8-IO w/o MF\\n1\\n0.452\\n0.929\\n0.988\\n1\\n16-IO w/ MF\\n1\\n0.738\\n0.762\\n0.988\\n0.952\\n16-IO w/o MF\\n1\\n0.190\\n0.964\\n1\\n1\\n8-shot SolverLearner\\n0.988\\n1\\n1\\n1\\n1\\nGPT-4\\nZero-shot\\n1\\n1\\n1\\n1\\n1\\n8-IO w/ MF\\n1\\n1\\n1\\n1\\n1\\n8-IO w/o MF\\n1\\n1\\n1\\n1\\n1\\n16-IO w/ MF\\n1\\n1\\n1\\n1\\n1\\n16-IO w/o MF\\n1\\n0.988\\n1\\n1\\n1\\n8-shot SolverLearner\\n1\\n1\\n1\\n1\\n1\\nTable 7: Full Main Results for Spatial Reasoning.\\nMethod\\nCoordinates\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270\\nRandom\\nGPT-3.5\\nZero-shot\\n0.273\\n0.702\\n0.143\\n0.012\\n0.310\\n0.060\\n0.024\\n8-IO w/ MF\\n0.952\\n0.845\\n0.869\\n0.25\\n0.976\\n0.060\\n0.095\\n8-IO w/o MF\\n0.369\\n0.726\\n0.310\\n0.083\\n0.690\\n0.107\\n0.071\\n16-IO w/ MF\\n0.929\\n0.893\\n0.857\\n0.274\\n0.952\\n0.071\\n0.131\\n16-IO w/o MF\\n0.452\\n0.667\\n0.452\\n0.083\\n0.798\\n0.131\\n0.083\\n8-shot SolverLearner\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\nGPT-4\\nZero-shot\\n0.119\\n0.060\\n0.083\\n0.024\\n0.048\\n0.012\\n0.036\\n8-IO w/ MF\\n1\\n1\\n0.964\\n0.643\\n0.952\\n0.679\\n0.190\\n8-IO w/o MF\\n1\\n0.976\\n0.929\\n0.560\\n0.976\\n0.429\\n0.333\\n16-IO w/ MF\\n1\\n1\\n0.952\\n0.690\\n0.929\\n0.667\\n0.214\\n16-IO w/o MF\\n1\\n0.976\\n0.964\\n0.607\\n0.976\\n0.405\\n0.369\\n8-shot SolverLearner\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\nTable 8: Full Main Results for Cipher Decryption.\\nMethod\\nEncryption System\\nAlphabetically Sorting Cipher\\nCaesar Cipher\\nMorse Cipher\\nGPT-3.5\\nZero-shot\\n0.560\\n0.036\\n0.512\\n8-IO w/ MF\\n0.595\\n0.024\\n0.464\\n8-IO w/o MF\\n0.560\\n0\\n0.452\\n16-IO w/ MF\\n0.619\\n0.024\\n0.536\\n16-IO w/o MF\\n0.512\\n0.012\\n0.440\\n8-shot SolverLearner\\n1\\n0\\n1\\nGPT-4\\nZero-shot\\n0.726\\n0\\n1\\n8-IO w/ MF\\n0.774\\n0.060\\n1\\n8-IO w/o MF\\n0.75\\n0.583\\n1\\n16-IO w/ MF\\n0.798\\n0.179\\n1\\n16-IO w/o MF\\n0.738\\n0.583\\n1\\n8-shot SolverLearner\\n1\\n1\\n1\\nTable 9: Results over Claude3 for Arithmetic Task.\\nMethod\\nBase\\n8\\n9\\n10\\n11\\n16\\nZero-shot\\n0.710\\n0.185\\n0.996\\n0.334\\n0.868\\n8-IO w/ MF\\n0.783\\n0.385\\n0.995\\n0.473\\n0.913\\n8-IO w/o MF\\n0.269\\n0.083\\n0.659\\n0.105\\n0.752\\n8-shot SolverLearner\\n0\\n0\\n1\\n0.095\\n1\\nTable 10: Results over Claude3 for Basic Syntactic Reasoning.\\nMethod\\nWord Order\\nOSV\\nOVS\\nSOV\\nVOS\\nVSO\\nZero-shot\\n1\\n1\\n1\\n1\\n0.988\\n8-IO w/ MF\\n1\\n1\\n1\\n1\\n1\\n8-IO w/o MF\\n1\\n0.976\\n1\\n1\\n1\\n8-shot SolverLearner\\n1\\n1\\n1\\n1\\n1\\nTable 11: Results over Claude3 for Spatial Reasoning.\\nMethod\\nCoordinates\\nDefault\\nR90\\nR180\\nR270\\nS-NS\\nS-WE\\nRandom\\nZero-shot\\n0.607\\n0.012\\n0.119\\n0.024\\n0.321\\n0.262\\n0.060\\n8-IO w/ MF\\n1\\n1\\n1\\n1\\n0.988\\n0.988\\n1\\n8-IO w/o MF\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n8-shot SolverLearner\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\nTable 12: Results over Claude3 for Cipher Decryption.\\nMethod\\nEncryption System\\nAlphabetically Sorting Cipher\\nCaesar Cipher\\nMorse Cipher\\nZero-shot\\n0.560\\n0.024\\n0.988\\n8-IO w/ MF\\n0.607\\n0.167\\n1\\n8-IO w/o MF\\n0.214\\n0.048\\n1\\n8-shot SolverLearner\\n0.131\\n0.119\\n1\\nTable 13: Results over the arithmetic task with Python interpreter as executor vs. GPT-3.5 as executor\\nExecutor\\nBase\\n8\\n9\\n10\\n11\\n16\\nPython Interpreter\\n1\\n1\\n1\\n1\\n1\\nGPT-3.5\\n0.398\\n0.196\\n0.934\\n0.152\\n0.64\\nTable 14: Results for the spatial reasoning over GPT-3.5 w.t.r the number of few-shot examples\\nShot\\nCoordinates\\nDefault\\nS-NS\\nS-WE\\nR90\\nR180\\nR270\\nRandom\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n2\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n4\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n8\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n16\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"reasoning\",\n",
    "    load_max_docs=2,\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Hunter × Hunter', 'summary': 'Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.', 'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter'}, page_content='Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.\\n\\n\\n== Synopsis ==\\n\\n\\n=== Setting ===\\nHunters (ハンター, Hantā) are licensed, elite members of humanity who are capable of tracking down secret treasures, rare beasts, or even other individuals, and can also access locations that regulars cannot. To obtain a license one must pass the rigorous annual Hunter Examination run by the Hunter Association, which has a success rate of less than one in a hundred-thousand. A Hunter may be awarded up to three stars: a single star for making \"remarkable achievements in a particular field\"; they may then be upgraded to two stars for \"holding an official position\" and mentoring another Hunter up to single star level; and finally upgraded to three stars for \"remarkable achievements in multiple fields\".\\nNen (念) is the ability to control one\\'s own life energy or aura, which is constantly emitted from them, knowingly or not. There are four basic Nen techniques: Ten (纏) maintains the aura in the body, strengthening it for defense; Zetsu (絕) shuts the aura flow off, useful for concealing one\\'s presence and relieving fatigue; Ren (練) enables a user to produce more Nen; and Hatsu (發) is a person\\'s specific use of Nen. Nen users are classified into six types based on their Hatsu abilities; Enhancers (強化系, Kyōkakei) strengthen and reinforce their natural physical abilities; Emitters (放出系, Hōshutsukei) project aura out of their bodies; Manipulators (操作系, Sōsakei) control objects or living things; Transmuters (変化系, Henkakei) change the type or properties of their aura; Conjurers (具現化系, Gugenkakei) create objects out of their aura; and Specialists (特質系, Tokushitsukei) have unique abilities that do not fall into the previous categories. A Nen user can enter into a Contract (誓約, Seiyaku) where, by pledging to follow certain Limitations (制約, Seiyaku), their abilities are strengthened in relation to how strict they are. An example of this is Kurapika who, in order to have an unbreakable chain that will fully restrain members of the Ph'),\n",
       " Document(metadata={'title': 'List of Hunter × Hunter characters', 'summary': \"The Hunter × Hunter manga series, created by Yoshihiro Togashi, features an extensive cast of characters. Such as Shihad Gandhi. It takes place in a fictional universe where licensed specialists known as Hunters travel the world taking on special jobs ranging from treasure hunting to assassination. The story initially focuses on Gon Freecss and his quest to become a Hunter in order to find his father, Ging, who is himself a famous Hunter. On the way, Gon meets and becomes close friends with Killua Zoldyck, Kurapika and Leorio Paradinight.\\nAlthough most characters are human, most possess superhuman strength and/or supernatural abilities due to Nen, the ability to control one's own life energy or aura. The world of the series also includes fantastical beasts such as the Chimera Ants or the Five great calamities.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/List_of_Hunter_%C3%97_Hunter_characters'}, page_content=\"The Hunter × Hunter manga series, created by Yoshihiro Togashi, features an extensive cast of characters. Such as Shihad Gandhi. It takes place in a fictional universe where licensed specialists known as Hunters travel the world taking on special jobs ranging from treasure hunting to assassination. The story initially focuses on Gon Freecss and his quest to become a Hunter in order to find his father, Ging, who is himself a famous Hunter. On the way, Gon meets and becomes close friends with Killua Zoldyck, Kurapika and Leorio Paradinight.\\nAlthough most characters are human, most possess superhuman strength and/or supernatural abilities due to Nen, the ability to control one's own life energy or aura. The world of the series also includes fantastical beasts such as the Chimera Ants or the Five great calamities.\\n\\n\\n== Protagonists ==\\n\\n\\n=== Gon Freecss ===\\n\\nVoiced by: Rica Matsumoto (1998 film), Junko Takeuchi (1999 series), Megumi Han (2011 series) (Japanese); Elinor Holt (1999 series), Erica Mendez (2011 series) (English)\\nGon Freecss (Japanese: ゴン=フリークス, Hepburn: Gon Furīkusu) is an athletic, naïve, and friendly boy. Having spent a lot of time in the woods as a child, he gets along very well with animals and has superhuman senses such as heightened sense of smell and sight, as well as very keen taste. Raised by Mito, Gon wants to become a Hunter in order to find his father, Ging, who is a Hunter as well. During the Hunter Exam Gon befriends Killua Zoldyck, Kurapika and Leorio Paradinight. After successfully becoming a licensed Hunter, Gon and Killua learn about Nen from Wing and later train further under Biscuit Krueger.[ch. 47, 48, 137] After becoming one of the first people to beat Greed Island, and helping to stop the Chimera Ants, Gon meets his father.[ch. 335]. Following the meeting with his father, Gon decides to return to Whale Island and reunites with Mito [ch. 345]. Gon is a popular character with fans, coming in third place in the series' first two popularity polls.\\n\\n\\n=== Killua Zoldyck ===\\n\\nVoiced by: Kanako Mitsuhashi (1999 series), Mariya Ise (2011 series) (Japanese); Annika Odegard (1999 series), Cristina Vee (2011 series) (English)\\nKillua Zoldyck (キルア=ゾルディック, Kirua Zorudikku) is initially introduced as a cheeky, cheerful and mischievous boy who befriends Gon during the Hunter Exam. His ruthlessness and aptitude in killing show the other side of him — deadly, violent, and bloodthirsty. A member of the famous Zoldyck Family of assassins, Killua has been trained to be an assassin since birth and conditioned to possess extreme tolerance for poison, electricity and overall pain. Although Killua fails during his first Hunter Exam by killing an opponent due to his elder brother Illumi's influence, he attends the exam again the following year, earning his license by eliminating all other applicants in the very first trial.[ch. 36, 37, 148] Killua and Gon learn about Nen from Wing and later train further under Biscuit Krueger.[ch. 47, 48, 137] He becomes one of the first people to beat Greed Island, helps stop the Chimera Ants, and uses his sibling Alluka's special abilities to heal the dying Gon. As Gon is about to meet Ging, Killua decides to part ways and travel the world with Alluka [ch. 338].\\nConflicting with Killua's predisposition to kill is his unyielding loyalty to his new friends, as Killua puts them before himself without a single complaint. His greatest flaw is initially his fear of those seemingly more powerful than him,[ch. 210] a result of Illumi's instructions to be extremely cautious and only engage in combat if victory is absolutely certain. Showing great promise from birth, he possesses extraordinary agility and strength as a one-man killing machine. Killua has mastered many killing techniques at a tender age and is set to be one of the best assassins his family has ever produced. His Nen type is Transmutation, which he utilizes by altering his aura into electricity. [ch. 60, 122] His Hatsu involves vari\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"HUNTER X HUNTER\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"If you have tried doing any form of important work that requires text analysis, natural language processing, and machine learning, you will soon find that text splitting is either going to make your analysis very effective or worse than even if you had never gone down that road at all.\n",
    "\n",
    "There are many different applications and use cases for this task but a more common hurdle you’ll run into is how to do this process of text splitting, most libraries have the chunk size and chunk overlap parameters to aid in this process, which is the subject of this article.\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n",
    "print(texts[1])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x11b07c6a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "sample = RecursiveCharacterTextSplitter(chunk_size=50,chunk_overlap=5)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "texts = sample.create_documents([txt])\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 If you have tried doing any form of important work\n",
      "41 work that requires text analysis, natural\n",
      "46 language processing, and machine learning, you\n",
      "48 you will soon find that text splitting is either\n",
      "45 going to make your analysis very effective or\n",
      "45 or worse than even if you had never gone down\n",
      "22 down that road at all.\n",
      "45 There are many different applications and use\n",
      "48 use cases for this task but a more common hurdle\n",
      "49 you’ll run into is how to do this process of text\n",
      "45 text splitting, most libraries have the chunk\n",
      "48 size and chunk overlap parameters to aid in this\n",
      "42 this process, which is the subject of this\n",
      "13 this article.\n",
      "7 texts =\n",
      "49 text_splitter.create_documents([state_of_the_unio\n",
      "8 _union])\n",
      "31 print(texts[0])\n",
      "print(texts[1])\n"
     ]
    }
   ],
   "source": [
    "for i in texts:\n",
    "    print(i.page_content.__len__(), i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'operationId': 'read_tracer_session_api_v1_sessions__session_id__get', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}': {'get': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'include_stats', 'in': 'query', 'required': False, 'schema': {'type': 'boolean', 'default': False, 'title': 'Include Stats'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n",
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"include_stats\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"boolean\", \"default\": false, \"title\": \"Include Stats\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "json_data\n",
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = splitter.split_json(json_data)\n",
    "\n",
    "for i in json_chunks[:3]:\n",
    "    print(i)\n",
    "\n",
    "#can also o/p the documents\n",
    "\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "for i in docs[:3]:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
